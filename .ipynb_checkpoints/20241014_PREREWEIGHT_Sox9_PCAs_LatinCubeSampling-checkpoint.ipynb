{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from _utils import *\n",
    "from scipy.stats import qmc\n",
    "\n",
    "def calculate_conformational_variance_jax(dmap_list, dmap_ref):\n",
    "    \"\"\"\n",
    "    Calculate the conformational variation of a set of distance maps relative to a reference map.\n",
    "\n",
    "    Parameters:\n",
    "    dmap_list (list): A list of 2D numpy arrays representing the distance maps.\n",
    "    dmap_ref (np.ndarray): A 2D numpy array representing the reference distance map.\n",
    "    num_probes (int): The number of probes in the distance maps.\n",
    "\n",
    "    Returns:\n",
    "    np.ndarray: A 2D numpy array containing the variance of the squared Euclidean distances \n",
    "               between each distance map and the reference map.\n",
    "    \"\"\"\n",
    "    # Convert dmap_list to a NumPy array\n",
    "    dmap_list = jnp.array(dmap_list)\n",
    "    \n",
    "    # Calculate the squared Euclidean distance between each distance map and the reference map\n",
    "    diff_list = jnp.sqrt((dmap_list - dmap_ref) ** 2) \n",
    "    \n",
    "    # Calculate the variance along the number of observation/cell dimension\n",
    "    var = jnp.var(diff_list, axis=0)\n",
    "    \n",
    "    return var\n",
    "\n",
    "\n",
    "# Rewrite this in a jax-compatible fashion\n",
    "from functools import partial\n",
    "@partial(jax.jit, static_argnums=(2,)) \n",
    "def batch_calculate_variances(dmap_list, dmap_ref, num_probes):\n",
    "    \"\"\"\n",
    "    Vectorized version that applies calculate_conformational_variance_jax across a batch of distance maps.\n",
    "    \"\"\"\n",
    "    return jax.vmap(lambda dmap: calculate_conformational_variance_jax(dmap_list, jnp.reshape(dmap, [num_probes, num_probes])))(dmap_ref)\n",
    "\n",
    "\n",
    "# Define the main loglikelihood function using JAX\n",
    "def loglikelihood_jax(dmap_flat, ref_dmap_flat, measurement_error, num_probes):\n",
    "    return jnp.sum(_loglikelihood_jax(dmap_flat, ref_dmap_flat, measurement_error, num_probes))\n",
    "\n",
    "\n",
    "# Define the helper function, with JAX-compatible logic\n",
    "def _loglikelihood_jax(dmap_flat, ref_dmap_flat, measurement_error, num_probes):\n",
    "    # Use lax.cond for control flow based on the condition\n",
    "    min_value = jnp.iinfo(jnp.int32).min\n",
    "    \n",
    "    def handle_invalid_reference(ref_dmap_flat):\n",
    "        # Return extremely low probability when ref_dmap_flat contains invalid values\n",
    "        return jnp.array([jnp.float32(min_value), jnp.float32(min_value)])\n",
    "    \n",
    "    def handle_valid_reference(ref_dmap_flat):\n",
    "        # Calculate the difference between distance map and reference \n",
    "        subtraction_map_sq = jnp.square(dmap_flat - ref_dmap_flat).reshape(num_probes, num_probes)\n",
    "\n",
    "        # Only consider the upper triangular part of the distance map\n",
    "        # because the diagonal values do not have variance\n",
    "        triu_indices = jnp.triu_indices(num_probes, k=1)\n",
    "        measurement_error_scaled = 2 * measurement_error[triu_indices]  # both triangles \n",
    "        subtraction_map_sq_scaled = 2 * subtraction_map_sq[triu_indices]  # both triangles\n",
    "        \n",
    "        # Calculate the normalization factor\n",
    "        normalization_factor = -jnp.sum(jnp.log(jnp.sqrt(2 * jnp.pi * measurement_error_scaled**2)))\n",
    "        \n",
    "        # Calculate the Gaussian term \n",
    "        gaussian_term = -jnp.sum(subtraction_map_sq_scaled / (2 * jnp.square(measurement_error_scaled)))\n",
    "        \n",
    "        return jnp.array([normalization_factor, gaussian_term])\n",
    "\n",
    "    # Apply the appropriate logic depending on whether ref_dmap_flat contains negative values\n",
    "    return lax.cond(\n",
    "        jnp.any(ref_dmap_flat <= -1),\n",
    "        handle_invalid_reference,\n",
    "        handle_valid_reference,\n",
    "        ref_dmap_flat\n",
    "    )\n",
    "    \n",
    "def compute_loglikelihood_for_y(y, templates_flatten, measurement_error_esc, num_probes):\n",
    "    return jax.vmap(lambda x, z: loglikelihood_jax(y, x, z, num_probes))(templates_flatten, measurement_error_esc)\n",
    "\n",
    "\n",
    "folder_path = '/mnt/home/tudomlumleart/ceph/05_Sox9Dataset/'\n",
    "    \n",
    "num_monomers = 80\n",
    "\n",
    "# Load polys data and then perform linear interpolation\n",
    "# List all .mat files in the folder and load them\n",
    "cnc_polys = scipy.io.loadmat(folder_path + 'cncPols.mat')['cncPols'][:num_monomers, :, :]\n",
    "esc_polys = scipy.io.loadmat(folder_path + 'escPols.mat')['escPols'][:num_monomers, :, :]\n",
    "\n",
    "esc_polys_interp = interpolate_polymers(esc_polys)\n",
    "cnc_polys_interp = interpolate_polymers(cnc_polys)\n",
    "\n",
    "def calculate_distance_map(polys):\n",
    "    # Extract the dimensions of the input array\n",
    "    num_probes, num_coords, num_cells = polys.shape\n",
    "    \n",
    "    # Initialize an array of the same shape to hold the interpolated values\n",
    "    new_maps = np.zeros((num_cells, num_probes, num_probes))\n",
    "    \n",
    "    # Iterate over each cell\n",
    "    for c in range(num_cells):\n",
    "        # Extract the data for the current cell\n",
    "        curr_cells = polys[:, :, c]\n",
    "        \n",
    "        # Skip cells with all missing values\n",
    "        if np.all(np.isnan(curr_cells)):\n",
    "            continue  # This leaves a matrix of zeros in the output array\n",
    "        \n",
    "        # Calculate the pairwise Euclidean distance between each pair of probes\n",
    "        dmap = squareform(pdist(curr_cells))\n",
    "        \n",
    "        # Assign the distance map to the corresponding position in the output array\n",
    "        new_maps[c, :, :] = dmap\n",
    "    \n",
    "    # Return the array with interpolated values\n",
    "    return new_maps\n",
    "\n",
    "esc_maps_interp = calculate_distance_map(esc_polys_interp)\n",
    "cnc_maps_interp = calculate_distance_map(cnc_polys_interp)\n",
    "esc_maps_interp_flat = np.array([x.flatten() for x in esc_maps_interp])\n",
    "cnc_maps_interp_flat = np.array([x.flatten() for x in cnc_maps_interp])\n",
    "all_maps_interp = np.concatenate((esc_maps_interp, cnc_maps_interp), axis=0)\n",
    "all_maps_interp_flat = np.concatenate((esc_maps_interp_flat, cnc_maps_interp_flat), axis=0)\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "pca.fit(all_maps_interp_flat)\n",
    "esc_maps_pca = pca.transform(esc_maps_interp_flat)\n",
    "cnc_maps_pca = pca.transform(cnc_maps_interp_flat)\n",
    "\n",
    "all_maps_pca = np.concatenate((esc_maps_pca, cnc_maps_pca), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "mcmc_dir = '/mnt/home/tudomlumleart/ceph/01_ChromatinEnsembleRefinement/chromatin-ensemble-refinement/MCMC_results/20241014_RunWeightMCMC_Sox9_PCA_LatinCubeSampling/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Latin Hypercube sampling to generate 1000 samples\n",
    "# This technique tries to sample data points in a way that they are evenly distributed\n",
    "\n",
    "num_microstates = 1000\n",
    "num_dimension = pca.n_components_  # pca.n_components_\n",
    "\n",
    "sampler = qmc.LatinHypercube(d=num_dimension)\n",
    "sample = sampler.random(n=num_microstates)\n",
    "\n",
    "# We can use quantiles to get physical bounds\n",
    "l_quantile = 0.01\n",
    "u_quantile = 1 - l_quantile\n",
    "l_bounds = np.quantile(all_maps_pca, l_quantile, axis=0)\n",
    "u_bounds = np.quantile(all_maps_pca, u_quantile, axis=0)\n",
    "sample_scaled = qmc.scale(sample, l_bounds, u_bounds)\n",
    "\n",
    "# Add these sample_scaled coordinates to a dataframe\n",
    "sample_df = pd.DataFrame(sample_scaled, columns=[f'PC{i+1}' for i in range(pca.n_components_)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save sample_df to a csv file\n",
    "# sample_df.to_csv(mcmc_dir + 'sampled_microstates.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inverse transform the samples\n",
    "microstates_maps = pca.inverse_transform(sample_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This causes jupyter notebook to crash!\n",
    "\n",
    "microstates_maps_jax = jnp.array(microstates_maps)\n",
    "esc_std = batch_calculate_variances(jnp.array(esc_maps_interp), microstates_maps_jax, num_monomers)**0.5\n",
    "cnc_std = batch_calculate_variances(jnp.array(cnc_maps_interp), microstates_maps_jax, num_monomers)**0.5"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jax_cuda118",
   "language": "python",
   "name": "jax_cuda118"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
