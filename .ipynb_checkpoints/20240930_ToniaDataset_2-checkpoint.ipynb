{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-14 17:58:48.312013: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-10-14 17:58:48.694240: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX512F AVX512_VNNI, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/mnt/sw/nix/store/gpkc8q6zjnp3n3h3w9hbmbj6gjbxs85w-python-3.10.10-view/lib/python3.10/site-packages/scipy/io/matlab/_mio.py:227: MatReadWarning: Duplicate variable name \"None\" in stream - replacing previous with new\n",
      "Consider mio5.varmats_from_mat to split file into single variable files\n",
      "  matfile_dict = MR.get_variables(variable_names)\n"
     ]
    }
   ],
   "source": [
    "from _utils import *\n",
    "\n",
    "# Load dataset from file\n",
    "folder_path = '/mnt/home/tudomlumleart/ceph/10_ToniaDataset/ToniaDataset_withPolys.mat'\n",
    "\n",
    "dataset = scipy.io.loadmat(folder_path)\n",
    "\n",
    "# Hox locus dataset\n",
    "nHox = 72\n",
    "ctcfNtPolysHox = dataset['ctcfNtPolysHox']\n",
    "ctcfPolysHox = dataset['ctcfPolysHox']\n",
    "ntPolysHox = dataset['ntPolysHox']\n",
    "radNtPolysHox = dataset['radNtPolysHox']\n",
    "radPolysHox = dataset['radPolysHox']\n",
    "\n",
    "# Sox locus dataset\n",
    "nSox = 93\n",
    "ctcfNtPolysSox = dataset['ctcfNtPolysSox']\n",
    "ctcfPolysSox = dataset['ctcfPolysSox']\n",
    "ntPolysSox = dataset['ntPolysSox']\n",
    "radNtPolysSox = dataset['radNtPolysSox']\n",
    "radPolysSox = dataset['radPolysSox']\n",
    "\n",
    "# Interpolate polymers to fill in the NaN values \n",
    "ctcfNtPolysHox = interpolate_polymers(ctcfNtPolysHox)\n",
    "ctcfPolysHox = interpolate_polymers(ctcfPolysHox)\n",
    "ntPolysHox = interpolate_polymers(ntPolysHox)\n",
    "radNtPolysHox = interpolate_polymers(radNtPolysHox)\n",
    "radPolysHox = interpolate_polymers(radPolysHox)\n",
    "\n",
    "ctcfNtPolysSox = interpolate_polymers(ctcfNtPolysSox)\n",
    "ctcfPolysSox = interpolate_polymers(ctcfPolysSox)\n",
    "ntPolysSox = interpolate_polymers(ntPolysSox)\n",
    "radNtPolysSox = interpolate_polymers(radNtPolysSox)\n",
    "radPolysSox = interpolate_polymers(radPolysSox)\n",
    "\n",
    "\n",
    "def calculate_distance_map(polys):\n",
    "    # Extract the dimensions of the input array\n",
    "    num_probes, num_coords, num_cells = polys.shape\n",
    "    \n",
    "    # Initialize an array of the same shape to hold the interpolated values\n",
    "    new_maps = np.zeros((num_cells, num_probes, num_probes))\n",
    "    \n",
    "    # Iterate over each cell\n",
    "    for c in range(num_cells):\n",
    "        # Extract the data for the current cell\n",
    "        curr_cells = polys[:, :, c]\n",
    "        \n",
    "        # Skip cells with all missing values\n",
    "        if np.all(np.isnan(curr_cells)):\n",
    "            continue  # This leaves a matrix of zeros in the output array\n",
    "        \n",
    "        # Calculate the pairwise Euclidean distance between each pair of probes\n",
    "        dmap = squareform(pdist(curr_cells))\n",
    "        \n",
    "        # Assign the distance map to the corresponding position in the output array\n",
    "        new_maps[c, :, :] = dmap\n",
    "    \n",
    "    # Return the array with interpolated values\n",
    "    return new_maps\n",
    "\n",
    "# Generate distance maps from interpolated polymers\n",
    "ctcfNtMapsHox = calculate_distance_map(ctcfNtPolysHox)\n",
    "ctcfMapsHox = calculate_distance_map(ctcfPolysHox)\n",
    "ntMapsHox = calculate_distance_map(ntPolysHox)\n",
    "radNtMapsHox = calculate_distance_map(radNtPolysHox)\n",
    "radMapsHox = calculate_distance_map(radPolysHox)\n",
    "\n",
    "ctcfNtMapsSox = calculate_distance_map(ctcfNtPolysSox)\n",
    "ctcfMapsSox = calculate_distance_map(ctcfPolysSox)\n",
    "ntMapsSox = calculate_distance_map(ntPolysSox)\n",
    "radNtMapsSox = calculate_distance_map(radNtPolysSox)\n",
    "radMapsSox = calculate_distance_map(radPolysSox)\n",
    "\n",
    "# Plot the median maps of these distance maps \n",
    "# To check if the distance maps are reasonable\n",
    "ctcfNtMapsHox_median = np.nanmedian(ctcfNtMapsHox, axis=0)\n",
    "ctcfMapsHox_median = np.nanmedian(ctcfMapsHox, axis=0)\n",
    "ntMapsHox_median = np.nanmedian(ntMapsHox, axis=0)\n",
    "radNtMapsHox_median = np.nanmedian(radNtMapsHox, axis=0)\n",
    "radMapsHox_median = np.nanmedian(radMapsHox, axis=0)\n",
    "\n",
    "ctcfNtMapsSox_median = np.nanmedian(ctcfNtMapsSox, axis=0)\n",
    "ctcfMapsSox_median = np.nanmedian(ctcfMapsSox, axis=0)\n",
    "ntMapsSox_median = np.nanmedian(ntMapsSox, axis=0)\n",
    "radNtMapsSox_median = np.nanmedian(radNtMapsSox, axis=0)\n",
    "radMapsSox_median = np.nanmedian(radMapsSox, axis=0)\n",
    "\n",
    "# Generate flatten maps from distance maps\n",
    "ctcfNtFlattenHox = np.array([x.flatten() for x in ctcfNtMapsHox])\n",
    "ctcfFlattenHox = np.array([x.flatten() for x in ctcfMapsHox])\n",
    "ntFlattenHox = np.array([x.flatten() for x in ntMapsHox])\n",
    "radNtFlattenHox = np.array([x.flatten() for x in radNtMapsHox])\n",
    "radFlattenHox = np.array([x.flatten() for x in radMapsHox])\n",
    "\n",
    "ctcfNtFlattenSox = np.array([x.flatten() for x in ctcfNtMapsSox])\n",
    "ctcfFlattenSox = np.array([x.flatten() for x in ctcfMapsSox])\n",
    "ntFlattenSox = np.array([x.flatten() for x in ntMapsSox])\n",
    "radNtFlattenSox = np.array([x.flatten() for x in radNtMapsSox])\n",
    "radFlattenSox = np.array([x.flatten() for x in radMapsSox])\n",
    "\n",
    "allFlattenHox = np.concatenate((ctcfNtFlattenHox, ctcfFlattenHox, ntFlattenHox, radNtFlattenHox, radFlattenHox), axis=0)\n",
    "\n",
    "pca_hox = PCA(n_components=2)\n",
    "pca_hox.fit(allFlattenHox)\n",
    "# Fit the PCA model to all Hox datasets\n",
    "ctcfNtHox = pca_hox.transform(ctcfNtFlattenHox)\n",
    "ctcfHox = pca_hox.transform(ctcfFlattenHox)\n",
    "ntHox = pca_hox.transform(ntFlattenHox)\n",
    "radNtHox = pca_hox.transform(radNtFlattenHox)\n",
    "radHox = pca_hox.transform(radFlattenHox)\n",
    "\n",
    "# Convert the principal components to a DataFrame\n",
    "ctcfNtHox_df = pd.DataFrame(ctcfNtHox, columns=['PC1', 'PC2'])\n",
    "ctcfNtHox_df['label'] = 'ctcfNtHox'\n",
    "ctcfHox_df = pd.DataFrame(ctcfHox, columns=['PC1', 'PC2'])\n",
    "ctcfHox_df['label'] = 'ctcfDegHox'\n",
    "ntHox_df = pd.DataFrame(ntHox, columns=['PC1', 'PC2'])\n",
    "ntHox_df['label'] = 'ntHox'\n",
    "radNtHox_df = pd.DataFrame(radNtHox, columns=['PC1', 'PC2'])\n",
    "radNtHox_df['label'] = 'radNtHox'\n",
    "radHox_df = pd.DataFrame(radHox, columns=['PC1', 'PC2'])\n",
    "radHox_df['label'] = 'radDegHox'\n",
    "\n",
    "all_df = pd.concat([ntHox_df, radNtHox_df, radHox_df, ctcfNtHox_df, ctcfHox_df], axis=0)\n",
    "\n",
    "# PCA for Sox locus\n",
    "allFlattenSox = np.concatenate((ctcfNtFlattenSox, ctcfFlattenSox, ntFlattenSox, radNtFlattenSox, radFlattenSox), axis=0)\n",
    "pca_sox = PCA(n_components=2)\n",
    "pca_sox.fit(allFlattenSox)\n",
    "# Fit the PCA model to all Sox datasets\n",
    "ctcfNtSox = pca_sox.transform(ctcfNtFlattenSox)\n",
    "ctcfSox = pca_sox.transform(ctcfFlattenSox)\n",
    "ntSox = pca_sox.transform(ntFlattenSox)\n",
    "radNtSox = pca_sox.transform(radNtFlattenSox)\n",
    "radSox = pca_sox.transform(radFlattenSox)\n",
    "\n",
    "# Convert the principal components to a DataFrame\n",
    "ctcfNtSox_df = pd.DataFrame(ctcfNtSox, columns=['PC1', 'PC2'])\n",
    "ctcfNtSox_df['label'] = 'ctcfNtSox'\n",
    "ctcfSox_df = pd.DataFrame(ctcfSox, columns=['PC1', 'PC2'])\n",
    "ctcfSox_df['label'] = 'ctcfDegSox'\n",
    "ntSox_df = pd.DataFrame(ntSox, columns=['PC1', 'PC2'])\n",
    "ntSox_df['label'] = 'ntSox'\n",
    "radNtSox_df = pd.DataFrame(radNtSox, columns=['PC1', 'PC2'])\n",
    "radNtSox_df['label'] = 'radNtSox'\n",
    "radSox_df = pd.DataFrame(radSox, columns=['PC1', 'PC2'])\n",
    "radSox_df['label'] = 'radDegSox'\n",
    "\n",
    "all_df_sox = pd.concat([ntSox_df, radNtSox_df, radSox_df, ctcfNtSox_df, ctcfSox_df], axis=0)\n",
    "\n",
    "# Find the lower and upper bounds of the PC1 and PC2 for Hox locus\n",
    "min_pc1_hox = min(all_df['PC1'])\n",
    "max_pc1_hox = max(all_df['PC1'])\n",
    "min_pc2_hox = min(all_df['PC2'])\n",
    "max_pc2_hox = max(all_df['PC2'])\n",
    "\n",
    "# Find the lower and upper bounds of the PC1 and PC2 for Sox locus\n",
    "min_pc1_sox = min(all_df_sox['PC1'])\n",
    "max_pc1_sox = max(all_df_sox['PC1'])\n",
    "min_pc2_sox = min(all_df_sox['PC2'])\n",
    "max_pc2_sox = max(all_df_sox['PC2'])\n",
    "\n",
    "def generate_microstates(min_pc1, max_pc1, min_pc2, max_pc2, num_microstates, pca_model):\n",
    "    \"\"\"\n",
    "    Generates a grid of points (microstates) based on provided PCA components ranges, \n",
    "    sorts them, and applies inverse transformation using the given PCA model.\n",
    "    \n",
    "    Parameters:\n",
    "    ----------\n",
    "    min_pc1 : float\n",
    "        Minimum value for the first principal component (PC1).\n",
    "    max_pc1 : float\n",
    "        Maximum value for the first principal component (PC1).\n",
    "    min_pc2 : float\n",
    "        Minimum value for the second principal component (PC2).\n",
    "    max_pc2 : float\n",
    "        Maximum value for the second principal component (PC2).\n",
    "    num_microstates : int\n",
    "        Number of microstates (grid points) to generate for each component.\n",
    "    pca_model : sklearn.decomposition.PCA\n",
    "        Pre-trained PCA model that will be used to inverse transform the generated grid points.\n",
    "    \n",
    "    Returns:\n",
    "    -------\n",
    "    np.ndarray\n",
    "        A NumPy array containing the inverse-transformed microstates.\n",
    "    \n",
    "    Example:\n",
    "    -------\n",
    "    microstates = generate_microstates(-5, 5, -5, 5, 75, pca_model)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create a grid of points\n",
    "    pc1 = np.linspace(min_pc1, max_pc1, num_microstates)\n",
    "    pc2 = np.linspace(min_pc2, max_pc2, num_microstates)\n",
    "    pc1, pc2 = np.meshgrid(pc1, pc2)\n",
    "    pc1 = pc1.flatten()\n",
    "    pc2 = pc2.flatten()\n",
    "\n",
    "    # Create a DataFrame from the grid points\n",
    "    grid = pd.DataFrame({'PC1': pc1, 'PC2': pc2})\n",
    "\n",
    "    # Sort PC2 in descending order while keeping PC1 in ascending order\n",
    "    grid_sorted = grid.sort_values(by=['PC1', 'PC2'], ascending=[True, False], ignore_index=True)\n",
    "\n",
    "    # Apply inverse transformation using the provided PCA model\n",
    "    microstates = pca_model.inverse_transform(grid_sorted)\n",
    "\n",
    "    return microstates\n",
    "\n",
    "# use 5 microstates for debugging\n",
    "num_microstates = 75\n",
    "\n",
    "pc1_hox = np.linspace(min_pc1_hox, max_pc1_hox, num_microstates)\n",
    "pc2_hox = np.linspace(min_pc2_hox, max_pc2_hox, num_microstates)\n",
    "pc1_hox, pc2_hox = np.meshgrid(pc1_hox, pc2_hox)\n",
    "pc1_hox = pc1_hox.flatten()\n",
    "pc2_hox = pc2_hox.flatten()\n",
    "\n",
    "# Put the grid points into a DataFrame\n",
    "grid_hox = pd.DataFrame({'PC1': pc1_hox, 'PC2': pc2_hox})\n",
    "# Sort PC2 in descending order while keeping PC1 in ascending order\n",
    "grid_hox = grid_hox.sort_values(by=['PC1', 'PC2'], ascending=[True, False], ignore_index=True)\n",
    "\n",
    "# Infer microstates from PCA for Hox locus\n",
    "microstates_hox = pca_hox.inverse_transform(grid_hox)\n",
    "\n",
    "# Use the function I just wrote to generate microstates for the Sox locus\n",
    "microstates_sox = generate_microstates(min_pc1_sox, max_pc1_sox, min_pc2_sox, max_pc2_sox, num_microstates, pca_sox)\n",
    "\n",
    "\n",
    "def calculate_conformational_variance_new(dmap_list, microstates_dmap):\n",
    "    # This is incorrect because it finds mean across all samples \n",
    "    \"\"\"\n",
    "    Calculate the conformational variation of a set of distance maps relative to a reference map.\n",
    "\n",
    "    Parameters:\n",
    "    dmap_list (list): A list of 2D numpy arrays representing the distance maps.\n",
    "    dmap_ref (np.ndarray): A 2D numpy array representing the reference distance map.\n",
    "    num_probes (int): The number of probes in the distance maps.\n",
    "\n",
    "    Returns:\n",
    "    np.ndarray: A 2D numpy array containing the variance of the squared Euclidean distances \n",
    "               between each distance map and the reference map.\n",
    "    \"\"\"\n",
    "    # Convert dmap_list to a NumPy array\n",
    "    dmap_list = np.array(dmap_list)\n",
    "    \n",
    "    num_microstates = microstates_dmap.shape[0]\n",
    "    num_probes = np.round(microstates_dmap.shape[1] ** 0.5).astype(int)\n",
    "    \n",
    "    dmap_list = dmap_list[:, np.newaxis, :]\n",
    "    microstates_dmap = microstates_dmap[np.newaxis, :, :]\n",
    "    \n",
    "    # Calculate the squared Euclidean distance between each distance map and the reference map\n",
    "    diff_list = np.sqrt((dmap_list - microstates_dmap) ** 2)\n",
    "    \n",
    "    # Calculate the variance along the number of observation/cell dimension\n",
    "    var = np.var(diff_list, axis=0)\n",
    "    \n",
    "    return np.reshape(var, (num_microstates, num_probes, num_probes))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_conformational_variance_jax(dmap_list, dmap_ref):\n",
    "    \"\"\"\n",
    "    Calculate the conformational variation of a set of distance maps relative to a reference map.\n",
    "\n",
    "    Parameters:\n",
    "    dmap_list (list): A list of 2D numpy arrays representing the distance maps.\n",
    "    dmap_ref (np.ndarray): A 2D numpy array representing the reference distance map.\n",
    "    num_probes (int): The number of probes in the distance maps.\n",
    "\n",
    "    Returns:\n",
    "    np.ndarray: A 2D numpy array containing the variance of the squared Euclidean distances \n",
    "               between each distance map and the reference map.\n",
    "    \"\"\"\n",
    "    # Convert dmap_list to a NumPy array\n",
    "    dmap_list = jnp.array(dmap_list)\n",
    "    \n",
    "    # Calculate the squared Euclidean distance between each distance map and the reference map\n",
    "    diff_list = jnp.sqrt((dmap_list - dmap_ref) ** 2) \n",
    "    \n",
    "    # Calculate the variance along the number of observation/cell dimension\n",
    "    var = jnp.var(diff_list, axis=0)\n",
    "    \n",
    "    return var\n",
    "\n",
    "# Rewrite this in a jax-compatible fashion\n",
    "from functools import partial\n",
    "@partial(jax.jit, static_argnums=(2,)) \n",
    "def batch_calculate_variances(dmap_list, dmap_ref, num_probes):\n",
    "    \"\"\"\n",
    "    Vectorized version that applies calculate_conformational_variance_jax across a batch of distance maps.\n",
    "    \"\"\"\n",
    "    return jax.vmap(lambda dmap: calculate_conformational_variance_jax(dmap_list, jnp.reshape(dmap, [num_probes, num_probes])))(dmap_ref)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "microstates_hox_jax = jnp.array(microstates_hox)\n",
    "\n",
    "# Calculate variances across all microstates using JAX\n",
    "# Faster but might run into GPU memory issues\n",
    "# this takes 1 minute\n",
    "ctcfNtHox_std = batch_calculate_variances(jnp.array(ctcfNtMapsHox), microstates_hox_jax, nHox)**0.5\n",
    "ctcfHox_std = batch_calculate_variances(jnp.array(ctcfMapsHox), microstates_hox_jax, nHox)**0.5\n",
    "ntHox_std = batch_calculate_variances(jnp.array(ntMapsHox), microstates_hox_jax, nHox)**0.5\n",
    "radNtHox_std = batch_calculate_variances(jnp.array(radNtMapsHox), microstates_hox_jax, nHox)**0.5\n",
    "radHox_std = batch_calculate_variances(jnp.array(radMapsHox), microstates_hox_jax, nHox)**0.5\n",
    "\n",
    "microstates_sox_jax = jnp.array(microstates_sox)\n",
    "\n",
    "ctcfNtSox_std = batch_calculate_variances(jnp.array(ctcfNtMapsSox), microstates_sox_jax, nSox)**0.5\n",
    "ctcfSox_std = batch_calculate_variances(jnp.array(ctcfMapsSox), microstates_sox_jax, nSox)**0.5\n",
    "ntSox_std = batch_calculate_variances(jnp.array(ntMapsSox), microstates_sox_jax, nSox)**0.5\n",
    "radNtSox_std = batch_calculate_variances(jnp.array(radNtMapsSox), microstates_sox_jax, nSox)**0.5\n",
    "radSox_std = batch_calculate_variances(jnp.array(radMapsSox), microstates_sox_jax, nSox)**0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 1.17 TiB for an array with shape (5505, 5625, 5184) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Calculate variances across all microstates \u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m ctcfNtHox_std \u001b[38;5;241m=\u001b[39m \u001b[43mcalculate_conformational_variance_new\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctcfNtFlattenHox\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmicrostates_hox\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m0.5\u001b[39m\n\u001b[1;32m      3\u001b[0m ctcfHox_std \u001b[38;5;241m=\u001b[39m calculate_conformational_variance_new(ctcfFlattenHox, microstates_hox)\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m0.5\u001b[39m\n\u001b[1;32m      4\u001b[0m ntHox_std \u001b[38;5;241m=\u001b[39m calculate_conformational_variance_new(ntFlattenHox, microstates_hox)\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m0.5\u001b[39m\n",
      "Cell \u001b[0;32mIn[15], line 258\u001b[0m, in \u001b[0;36mcalculate_conformational_variance_new\u001b[0;34m(dmap_list, microstates_dmap)\u001b[0m\n\u001b[1;32m    255\u001b[0m microstates_dmap \u001b[38;5;241m=\u001b[39m microstates_dmap[np\u001b[38;5;241m.\u001b[39mnewaxis, :, :]\n\u001b[1;32m    257\u001b[0m \u001b[38;5;66;03m# Calculate the squared Euclidean distance between each distance map and the reference map\u001b[39;00m\n\u001b[0;32m--> 258\u001b[0m diff_list \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msqrt((\u001b[43mdmap_list\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmicrostates_dmap\u001b[49m) \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    260\u001b[0m \u001b[38;5;66;03m# Calculate the variance along the number of observation/cell dimension\u001b[39;00m\n\u001b[1;32m    261\u001b[0m var \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mvar(diff_list, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "\u001b[0;31mMemoryError\u001b[0m: Unable to allocate 1.17 TiB for an array with shape (5505, 5625, 5184) and data type float64"
     ]
    }
   ],
   "source": [
    "# Calculate variances across all microstates \n",
    "# this require bigmem because it requires big memory\n",
    "ctcfNtHox_std = calculate_conformational_variance_new(ctcfNtFlattenHox, microstates_hox)**0.5\n",
    "ctcfHox_std = calculate_conformational_variance_new(ctcfFlattenHox, microstates_hox)**0.5\n",
    "ntHox_std = calculate_conformational_variance_new(ntFlattenHox, microstates_hox)**0.5\n",
    "radNtHox_std = calculate_conformational_variance_new(radNtFlattenHox, microstates_hox)**0.5\n",
    "radHox_std = calculate_conformational_variance_new(radFlattenHox, microstates_hox)**0.5\n",
    "\n",
    "ctcfNtSox_std = calculate_conformational_variance_new(ctcfNtFlattenSox, microstates_sox)**0.5\n",
    "ctcfSox_std = calculate_conformational_variance_new(ctcfFlattenSox, microstates_sox)**0.5\n",
    "ntSox_std = calculate_conformational_variance_new(ntFlattenSox, microstates_sox)**0.5\n",
    "radNtSox_std = calculate_conformational_variance_new(radNtFlattenSox, microstates_sox)**0.5\n",
    "radSox_std = calculate_conformational_variance_new(radFlattenSox, microstates_sox)**0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "lpm_hox = [(logprior(x, nHox)).tolist() for x in microstates_hox]\n",
    "lpm_sox = [(logprior(x, nSox)).tolist() for x in microstates_sox]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loglikelihood(observed_dmap_flat, microstates_dmap_flat, measurement_error, num_probes):\n",
    "    \"\"\" \n",
    "    \"\"\"\n",
    "    num_microstates = microstates_dmap_flat.shape[0]\n",
    "    num_observations = observed_dmap_flat.shape[0]\n",
    "    \n",
    "    # Append a new axis for broadcasting\n",
    "    observed_dmap_flat = observed_dmap_flat[np.newaxis, :, :]\n",
    "    microstates_dmap_flat = microstates_dmap_flat[:, np.newaxis, :]\n",
    "    measurement_error = measurement_error[:, np.newaxis, :, :]\n",
    "        \n",
    "    \n",
    "    # Calculate the difference between distance map and reference \n",
    "    # distance map\n",
    "    subtraction_map_sq = np.square(observed_dmap_flat - microstates_dmap_flat).reshape(num_microstates, num_observations, \n",
    "                                                                      num_probes, num_probes)\n",
    "\n",
    "    # Only consider the upper triangular part of the distance map\n",
    "    triu_indices = np.triu_indices(num_probes, k=1)\n",
    "    measurement_error = 2*measurement_error[:, :, triu_indices[0], triu_indices[1]]  # both triangles \n",
    "    subtraction_map_sq = 2*subtraction_map_sq[:, :, triu_indices[0], triu_indices[1]]  # both triangles\n",
    "    \n",
    "    # Calculate the normalization factor\n",
    "    normalization_factor = -np.sum(np.log(np.sqrt(2*np.pi*measurement_error**2)), axis=-1)\n",
    "    \n",
    "    # Calculate the gaussian term \n",
    "    gaussian_term = -np.sum(subtraction_map_sq/(2*np.square(measurement_error)), axis=-1)\n",
    "    \n",
    "    # if the reference distance map is not physical ie contains negative values\n",
    "    # return very low probability: the lowest number numpy can handle\n",
    "    if np.any(microstates_dmap_flat <= -1):\n",
    "        unphysical_microstates_indices = np.squeeze(np.any(microstates_dmap_flat < 0, axis=-1))\n",
    "        # print(unphysical_microstates_indices.shape)\n",
    "        normalization_factor[unphysical_microstates_indices] = np.iinfo(np.int32).min\n",
    "        gaussian_term[unphysical_microstates_indices] = np.iinfo(np.int32).min\n",
    "    \n",
    "    # Change the dimension so it is compatible with the downstream analysis\n",
    "    return np.transpose(normalization_factor + gaussian_term)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e316d6f0422941a181922e865d411c93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5505 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "200f1bffc208455dab8119e512eb85a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8737 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d9280c35c254e8282486c659207d0de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/15319 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03f3c4928253439fbe278e7ac3f810ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7855 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "431bf8d19b18468bb868df3597b6c4ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7462 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "756cdfcb08a44bc6a98bc30f05b2e1c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5451 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87b704330a714538b4ec2d99b4422ed0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7902 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a99367995754757b7ff14ea4e9bfa78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11900 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0fc7d439c0fb4c279e87409d7dece3a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4094 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8758ca7fbb5b4a64b075e676c457ac3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8090 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define the main loglikelihood function using JAX\n",
    "def loglikelihood_jax(dmap_flat, ref_dmap_flat, measurement_error, num_probes):\n",
    "    return jnp.sum(_loglikelihood_jax(dmap_flat, ref_dmap_flat, measurement_error, num_probes))\n",
    "\n",
    "# Define the helper function, with JAX-compatible logic\n",
    "def _loglikelihood_jax(dmap_flat, ref_dmap_flat, measurement_error, num_probes):\n",
    "    # Use lax.cond for control flow based on the condition\n",
    "    min_value = jnp.iinfo(jnp.int32).min\n",
    "    \n",
    "    def handle_invalid_reference(ref_dmap_flat):\n",
    "        # Return extremely low probability when ref_dmap_flat contains invalid values\n",
    "        return jnp.array([jnp.float32(min_value), jnp.float32(min_value)])\n",
    "    \n",
    "    def handle_valid_reference(ref_dmap_flat):\n",
    "        # Calculate the difference between distance map and reference \n",
    "        subtraction_map_sq = jnp.square(dmap_flat - ref_dmap_flat).reshape(num_probes, num_probes)\n",
    "\n",
    "        # Only consider the upper triangular part of the distance map\n",
    "        # because the diagonal values do not have variance\n",
    "        triu_indices = jnp.triu_indices(num_probes, k=1)\n",
    "        measurement_error_scaled = 2 * measurement_error[triu_indices]  # both triangles \n",
    "        subtraction_map_sq_scaled = 2 * subtraction_map_sq[triu_indices]  # both triangles\n",
    "        \n",
    "        # Calculate the normalization factor\n",
    "        normalization_factor = -jnp.sum(jnp.log(jnp.sqrt(2 * jnp.pi * measurement_error_scaled**2)))\n",
    "        \n",
    "        # Calculate the Gaussian term \n",
    "        gaussian_term = -jnp.sum(subtraction_map_sq_scaled / (2 * jnp.square(measurement_error_scaled)))\n",
    "        \n",
    "        return jnp.array([normalization_factor, gaussian_term])\n",
    "\n",
    "    # Apply the appropriate logic depending on whether ref_dmap_flat contains negative values\n",
    "    return lax.cond(\n",
    "        jnp.any(ref_dmap_flat <= -1),\n",
    "        handle_invalid_reference,\n",
    "        handle_valid_reference,\n",
    "        ref_dmap_flat\n",
    "    )\n",
    "    \n",
    "def compute_loglikelihood_for_y(y, templates_flatten, measurement_error_esc, num_probes):\n",
    "    return jax.vmap(lambda x, z: loglikelihood_jax(y, x, z, num_probes))(templates_flatten, measurement_error_esc)\n",
    "\n",
    "# Calculate likelihood for Hox samples\n",
    "ctcfNtHox_ll = [compute_loglikelihood_for_y(y, microstates_hox_jax, ctcfNtHox_std, nHox) for y in tqdm(ctcfNtFlattenHox)] \n",
    "ctcfHox_ll = [compute_loglikelihood_for_y(y, microstates_hox_jax, ctcfHox_std, nHox) for y in tqdm(ctcfFlattenHox)]\n",
    "ntHox_ll = [compute_loglikelihood_for_y(y, microstates_hox_jax, ntHox_std, nHox) for y in tqdm(ntFlattenHox)]\n",
    "radNtHox_ll = [compute_loglikelihood_for_y(y, microstates_hox_jax, radNtHox_std, nHox) for y in tqdm(radNtFlattenHox)]\n",
    "radHox_ll = [compute_loglikelihood_for_y(y, microstates_hox_jax, radHox_std, nHox) for y in tqdm(radFlattenHox)]\n",
    "\n",
    "# Calculate likelihood for Sox samples\n",
    "ctcfNtSox_ll = [compute_loglikelihood_for_y(y, microstates_sox_jax, ctcfNtSox_std, nSox) for y in tqdm(ctcfNtFlattenSox)]\n",
    "ctcfSox_ll = [compute_loglikelihood_for_y(y, microstates_sox_jax, ctcfSox_std, nSox) for y in tqdm(ctcfFlattenSox)]\n",
    "ntSox_ll = [compute_loglikelihood_for_y(y, microstates_sox_jax, ntSox_std, nSox) for y in tqdm(ntFlattenSox)]\n",
    "radNtSox_ll = [compute_loglikelihood_for_y(y, microstates_sox_jax, radNtSox_std, nSox) for y in tqdm(radNtFlattenSox)]\n",
    "radSox_ll = [compute_loglikelihood_for_y(y, microstates_sox_jax, radSox_std, nSox) for y in tqdm(radFlattenSox)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the helper function, with JAX-compatible logic\n",
    "# run into a memory issue \n",
    "def loglikelihood_jax_new(observed_dmap_flat, microstates_dmap_flat, measurement_error, num_probes):\n",
    "    num_microstates = microstates_dmap_flat.shape[0]\n",
    "    num_observations = observed_dmap_flat.shape[0]\n",
    "    \n",
    "    # Append a new axis for broadcasting\n",
    "    observed_dmap_flat = observed_dmap_flat[None, :, :]\n",
    "    microstates_dmap_flat = microstates_dmap_flat[:, None, :]\n",
    "    measurement_error = measurement_error[:, None, :, :]\n",
    "    \n",
    "    # Calculate the difference between distance map and reference \n",
    "    # distance map\n",
    "    subtraction_map_sq = jnp.square(observed_dmap_flat - microstates_dmap_flat).reshape(num_microstates, num_observations, \n",
    "                                                                      num_probes, num_probes)\n",
    "    \n",
    "    # Calculate the normalization factor\n",
    "    normalization_factor = -jnp.sum(jnp.log(jnp.sqrt(2 * jnp.pi * measurement_error_scaled**2)), axis=-1)\n",
    "    \n",
    "    # Calculate the Gaussian term \n",
    "    gaussian_term = -jnp.sum(subtraction_map_sq_scaled / (2 * jnp.square(measurement_error_scaled)), axis=-1)\n",
    "\n",
    "    # Only consider the upper triangular part of the distance map\n",
    "    # because the diagonal values do not have variance\n",
    "    triu_indices = jnp.triu_indices(num_probes, k=1)\n",
    "    measurement_error_scaled = 2 * measurement_error[triu_indices]  # both triangles \n",
    "    subtraction_map_sq_scaled = 2 * subtraction_map_sq[triu_indices]  # both triangles\n",
    "   \n",
    "    # Check for unphysical microstates (i.e., negative values in the distance map)\n",
    "    unphysical_microstates_indices = jnp.any(microstates_dmap_flat < 0, axis=-1)\n",
    "\n",
    "    # Get the minimum integer value that jax.numpy can handle\n",
    "    min_value = jnp.iinfo(jnp.int32).min\n",
    "\n",
    "    # Replace the values of `normalization_factor` and `gaussian_term` for unphysical indices\n",
    "    normalization_factor = jnp.where(unphysical_microstates_indices, min_value, normalization_factor)\n",
    "    gaussian_term = jnp.where(unphysical_microstates_indices, min_value, gaussian_term)\n",
    "\n",
    "    return jnp.transpose(normalization_factor + gaussian_term)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 1.17 TiB for an array with shape (5625, 5505, 5184) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m ctcfNtHox_ll_jn \u001b[38;5;241m=\u001b[39m \u001b[43mloglikelihood_jax_new\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctcfNtFlattenHox\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmicrostates_hox\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctcfNtHox_std\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnHox\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[12], line 13\u001b[0m, in \u001b[0;36mloglikelihood_jax_new\u001b[0;34m(observed_dmap_flat, microstates_dmap_flat, measurement_error, num_probes)\u001b[0m\n\u001b[1;32m      9\u001b[0m measurement_error \u001b[38;5;241m=\u001b[39m measurement_error[:, \u001b[38;5;28;01mNone\u001b[39;00m, :, :]\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Calculate the difference between distance map and reference \u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# distance map\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m subtraction_map_sq \u001b[38;5;241m=\u001b[39m jnp\u001b[38;5;241m.\u001b[39msquare(\u001b[43mobserved_dmap_flat\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmicrostates_dmap_flat\u001b[49m)\u001b[38;5;241m.\u001b[39mreshape(num_microstates, num_observations, \n\u001b[1;32m     14\u001b[0m                                                                   num_probes, num_probes)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Calculate the normalization factor\u001b[39;00m\n\u001b[1;32m     17\u001b[0m normalization_factor \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39mjnp\u001b[38;5;241m.\u001b[39msum(jnp\u001b[38;5;241m.\u001b[39mlog(jnp\u001b[38;5;241m.\u001b[39msqrt(\u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m jnp\u001b[38;5;241m.\u001b[39mpi \u001b[38;5;241m*\u001b[39m measurement_error_scaled\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m)), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mMemoryError\u001b[0m: Unable to allocate 1.17 TiB for an array with shape (5625, 5505, 5184) and data type float64"
     ]
    }
   ],
   "source": [
    "ctcfNtHox_ll_jn = loglikelihood_jax_new(ctcfNtFlattenHox, microstates_hox, ctcfNtHox_std, nHox)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 1.17 TiB for an array with shape (5625, 5505, 5184) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# This takes about 20 seconds to run each sample\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# JAX might be faster \u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m ctcfNtHox_ll \u001b[38;5;241m=\u001b[39m \u001b[43mloglikelihood\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctcfNtFlattenHox\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmicrostates_hox\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctcfNtHox_std\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnHox\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m ctcfHox_ll \u001b[38;5;241m=\u001b[39m loglikelihood(ctcfFlattenHox, microstates_hox, ctcfHox_std, nHox)\n\u001b[1;32m      6\u001b[0m ntHox_ll \u001b[38;5;241m=\u001b[39m loglikelihood(ntFlattenHox, microstates_hox, ntHox_std, nHox)\n",
      "Cell \u001b[0;32mIn[6], line 15\u001b[0m, in \u001b[0;36mloglikelihood\u001b[0;34m(observed_dmap_flat, microstates_dmap_flat, measurement_error, num_probes)\u001b[0m\n\u001b[1;32m     10\u001b[0m measurement_error \u001b[38;5;241m=\u001b[39m measurement_error[:, np\u001b[38;5;241m.\u001b[39mnewaxis, :, :]\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Calculate the difference between distance map and reference \u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# distance map\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m subtraction_map_sq \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msquare(\u001b[43mobserved_dmap_flat\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmicrostates_dmap_flat\u001b[49m)\u001b[38;5;241m.\u001b[39mreshape(num_microstates, num_observations, \n\u001b[1;32m     16\u001b[0m                                                                   num_probes, num_probes)\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Only consider the upper triangular part of the distance map\u001b[39;00m\n\u001b[1;32m     19\u001b[0m triu_indices \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mtriu_indices(num_probes, k\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mMemoryError\u001b[0m: Unable to allocate 1.17 TiB for an array with shape (5625, 5505, 5184) and data type float64"
     ]
    }
   ],
   "source": [
    "# This takes about 20 seconds to run each sample\n",
    "# JAX might be faster \n",
    "# requires bigmemory \n",
    "\n",
    "ctcfNtHox_ll = loglikelihood(ctcfNtFlattenHox, microstates_hox, ctcfNtHox_std, nHox)\n",
    "ctcfHox_ll = loglikelihood(ctcfFlattenHox, microstates_hox, ctcfHox_std, nHox)\n",
    "ntHox_ll = loglikelihood(ntFlattenHox, microstates_hox, ntHox_std, nHox)\n",
    "radNtHox_ll = loglikelihood(radNtFlattenHox, microstates_hox, radNtHox_std, nHox)\n",
    "radHox_ll = loglikelihood(radFlattenHox, microstates_hox, radHox_std, nHox)\n",
    "\n",
    "ctcfNtSox_ll = loglikelihood(ctcfNtFlattenSox, microstates_sox, ctcfNtSox_std, nSox)\n",
    "ctcfSox_ll = loglikelihood(ctcfFlattenSox, microstates_sox, ctcfSox_std, nSox)\n",
    "ntSox_ll = loglikelihood(ntFlattenSox, microstates_sox, ntSox_std, nSox)\n",
    "radNtSox_ll = loglikelihood(radNtFlattenSox, microstates_sox, radNtSox_std, nSox)\n",
    "radSox_ll = loglikelihood(radFlattenSox, microstates_sox, radSox_std, nSox)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of CPU cores: 64\n",
      "Number of threads per chain: 16\n"
     ]
    }
   ],
   "source": [
    "# Convert all jnp arrays to list \n",
    "ctcfNtHox_ll = [x.tolist() for x in ctcfNtHox_ll]\n",
    "ctcfHox_ll = [x.tolist() for x in ctcfHox_ll]\n",
    "ntHox_ll = [x.tolist() for x in ntHox_ll]\n",
    "radNtHox_ll = [x.tolist() for x in radNtHox_ll]\n",
    "radHox_ll = [x.tolist() for x in radHox_ll]\n",
    "\n",
    "ctcfNtSox_ll = [x.tolist() for x in ctcfNtSox_ll]\n",
    "ctcfSox_ll = [x.tolist() for x in ctcfSox_ll]\n",
    "ntSox_ll = [x.tolist() for x in ntSox_ll]\n",
    "radNtSox_ll = [x.tolist() for x in radNtSox_ll]\n",
    "radSox_ll = [x.tolist() for x in radSox_ll]\n",
    "\n",
    "# Calculate the number of cells in each dataset\n",
    "N_ctcfNtHox = ctcfNtMapsHox.shape[0]\n",
    "N_ctcfHox = ctcfMapsHox.shape[0]\n",
    "N_ntHox = ntMapsHox.shape[0]\n",
    "N_radNtHox = radNtMapsHox.shape[0]\n",
    "N_radHox = radMapsHox.shape[0]\n",
    "\n",
    "N_ctcfNtSox = ctcfNtMapsSox.shape[0]\n",
    "N_ctcfSox = ctcfMapsSox.shape[0]\n",
    "N_ntSox = ntMapsSox.shape[0]\n",
    "N_radNtSox = radNtMapsSox.shape[0]\n",
    "N_radSox = radMapsSox.shape[0]\n",
    "\n",
    "M = num_microstates**2  # Number of microstates\n",
    "\n",
    "# Load stan model \n",
    "my_model = CmdStanModel(\n",
    "    stan_file='/mnt/home/tudomlumleart/ceph/01_ChromatinEnsembleRefinement/chromatin-ensemble-refinement/scripts/stan/20240715_WeightOptimization.stan',\n",
    "    cpp_options = {\n",
    "        \"STAN_THREADS\": True,\n",
    "    }\n",
    "    )\n",
    "\n",
    "n_cores = multiprocessing.cpu_count()\n",
    "print(f\"Number of CPU cores: {n_cores}\")\n",
    "parallel_chains = 4\n",
    "threads_per_chain = int(n_cores / parallel_chains)\n",
    "print(f\"Number of threads per chain: {threads_per_chain}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7bde495be6e74665a703896ceca85356",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "save_dir = '/mnt/home/tudomlumleart/ceph/01_ChromatinEnsembleRefinement/chromatin-ensemble-refinement/MCMC_results/20240930_RunWeightMCMC_Tonia_PCA_2'\n",
    "conditions = [\n",
    "    'ctcfNtHox', 'ctcfHox', 'ntHox', 'radNtHox', 'radHox',\n",
    "    'ctcfNtSox', 'ctcfSox', 'ntSox', 'radNtSox', 'radSox'\n",
    "]\n",
    "\n",
    "for condition in tqdm(conditions):\n",
    "    output_dir = os.path.join(save_dir, condition)\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    \n",
    "    json_filename = os.path.join(output_dir, 'data.json')\n",
    "    stan_putput_file = os.path.join(output_dir, 'stan_output')\n",
    "    \n",
    "    # Generate a datadict for the current condition\n",
    "    if condition == 'ctcfNtHox':\n",
    "        data_dict = {\n",
    "            'N': N_ctcfNtHox,\n",
    "            'M': M,\n",
    "            'll_map': ctcfNtHox_ll,\n",
    "            'lpm_vec': lpm_hox\n",
    "        }\n",
    "    elif condition == 'ctcfHox':\n",
    "        data_dict = {\n",
    "            'N': N_ctcfHox,\n",
    "            'M': M,\n",
    "            'll_map': ctcfHox_ll,\n",
    "            'lpm_vec': lpm_hox\n",
    "        }\n",
    "    elif condition == 'ntHox':\n",
    "        data_dict = {\n",
    "            'N': N_ntHox,\n",
    "            'M': M,\n",
    "            'll_map': ntHox_ll,\n",
    "            'lpm_vec': lpm_hox\n",
    "        }\n",
    "    elif condition == 'radNtHox':\n",
    "        data_dict = {\n",
    "            'N': N_radNtHox,\n",
    "            'M': M,\n",
    "            'll_map': radNtHox_ll,\n",
    "            'lpm_vec': lpm_hox\n",
    "        }\n",
    "    elif condition == 'radHox':\n",
    "        data_dict = {\n",
    "            'N': N_radHox,\n",
    "            'M': M,\n",
    "            'll_map': radHox_ll,\n",
    "            'lpm_vec': lpm_hox\n",
    "        }\n",
    "    elif condition == 'ctcfNtSox':\n",
    "        data_dict = {\n",
    "            'N': N_ctcfNtSox,\n",
    "            'M': M,\n",
    "            'll_map': ctcfNtSox_ll,\n",
    "            'lpm_vec': lpm_sox\n",
    "        }\n",
    "    elif condition == 'ctcfSox':\n",
    "        data_dict = {\n",
    "            'N': N_ctcfSox,\n",
    "            'M': M,\n",
    "            'll_map': ctcfSox_ll,\n",
    "            'lpm_vec': lpm_sox\n",
    "        }\n",
    "    elif condition == 'ntSox':\n",
    "        data_dict = {\n",
    "            'N': N_ntSox,\n",
    "            'M': M,\n",
    "            'll_map': ntSox_ll,\n",
    "            'lpm_vec': lpm_sox\n",
    "        }\n",
    "    elif condition == 'radNtSox':\n",
    "        data_dict = {\n",
    "            'N': N_radNtSox,\n",
    "            'M': M,\n",
    "            'll_map': radNtSox_ll,\n",
    "            'lpm_vec': lpm_sox\n",
    "        }\n",
    "    elif condition == 'radSox':\n",
    "        data_dict = {\n",
    "            'N': N_radSox,\n",
    "            'M': M,\n",
    "            'll_map': radSox_ll,\n",
    "            'lpm_vec': lpm_sox\n",
    "        }\n",
    "    \n",
    "    json_obj = json.dumps(data_dict, indent=4)\n",
    "    \n",
    "    with open(json_filename, 'w') as json_file:\n",
    "        json_file.write(json_obj)\n",
    "        json_file.close()\n",
    "    \n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyter-gpu-openmm",
   "language": "python",
   "name": "jupyter-gpu-openmm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
