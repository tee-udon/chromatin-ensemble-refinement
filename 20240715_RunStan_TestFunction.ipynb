{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load necessary modules and dataset \n",
    "import sys\n",
    "sys.path.append(r\"/mnt/ceph/users/tudomlumleart/00_VirtualEnvironments/teeu/lib/python3.10/site-packages\")\n",
    "sys.path.append(r\"/mnt/home/tudomlumleart/.local/lib/python3.10/site-packages/\")\n",
    "sys.path.append(r\"/mnt/home/tudomlumleart/ceph/00_VirtualEnvironments/jupyter-gpu/lib/python3.10/site-packages\")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from functions import *\n",
    "from utils import *\n",
    "\n",
    "import torch\n",
    "import json\n",
    "import multiprocessing\n",
    "from cmdstanpy import CmdStanModel\n",
    "\n",
    "def logprior(dmap_flat, num_probes):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    return np.sum(np.array(logprior_(dmap_flat, num_probes)))\n",
    "\n",
    "def logprior_(dmap_flat, num_probes):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    # Get 2D map back to simplify the expression \n",
    "    dmap = np.reshape(dmap_flat, [num_probes, num_probes])\n",
    "    \n",
    "    # Calculate the squared end-to-end distance \n",
    "    R_sq = dmap[0][-1] ** 2\n",
    "    \n",
    "    # Calculate the average bond length\n",
    "    b = np.mean(np.diag(dmap, 1))\n",
    "    \n",
    "    N = num_probes\n",
    "    \n",
    "    # Calculate the probability\n",
    "    scaling_factor = 1.5 * np.log(3/(2*np.pi*N*b**2))\n",
    "    gaussian_term = -3*R_sq/(2*N*b**2)\n",
    "    \n",
    "    return scaling_factor, gaussian_term \n",
    "\n",
    "\n",
    "def loglikelihood(dmap_flat, ref_dmap_flat, measurement_error, num_probes):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    return np.sum(np.array(loglikelihood_(dmap_flat, ref_dmap_flat, measurement_error, num_probes)))\n",
    "\n",
    "\n",
    "def loglikelihood_(dmap_flat, ref_dmap_flat, measurement_error, num_probes):\n",
    "    \"\"\" \n",
    "    \"\"\"\n",
    "    # Calculate the difference between distance map and reference \n",
    "    # distance map\n",
    "    subtraction_map_sq = np.square(dmap_flat - ref_dmap_flat)\n",
    "    sum_subtraction_map_sq = np.sum(subtraction_map_sq)\n",
    "    \n",
    "    # Calculate the normalization factor\n",
    "    normalization_factor = -np.square(num_probes) * np.log(np.sqrt(2*np.pi*np.square(measurement_error)))\n",
    "    \n",
    "    # Calculate the gaussian term \n",
    "    gaussian_term = -np.sum(sum_subtraction_map_sq)/(2*np.square(measurement_error))\n",
    "    \n",
    "    return normalization_factor, gaussian_term\n",
    "\n",
    "def run_mcmc(dataset_list, param_dict, save_dir, dataset_idx):\n",
    "    # Unpack the param dict \n",
    "    num_monomers = param_dict['num_monomers']\n",
    "    mean_bond_length = param_dict['mean_bond_length']\n",
    "    std_bond_length = param_dict['std_bond_length'] \n",
    "    num_templates = param_dict['num_templates']\n",
    "    measurement_error = param_dict['noise_std']\n",
    "    weight_dist = param_dict['weights_dist']\n",
    "    num_observations = param_dict['num_observations']\n",
    "    num_probes = num_monomers\n",
    "    num_candidates = num_templates\n",
    "    \n",
    "    # Generate variables for the optimization\n",
    "    template_list = dataset_list[dataset_idx]['template_chain_list']\n",
    "    X = dataset_list[dataset_idx]['observation_list'][:num_observations]\n",
    "    label_list = dataset_list[dataset_idx]['labels']\n",
    "    observation_flatten_list = [squareform(pdist(x)).flatten() for x in X]\n",
    "    \n",
    "    # generate weight of each label from label_list\n",
    "    true_weights = np.array([np.sum(label_list == i) for i in np.unique(label_list)]) / len(label_list)\n",
    "    true_weights = true_weights.reshape(-1, 1)\n",
    "    templates_flatten = [squareform(pdist(x)).flatten() for x in template_list]\n",
    "    \n",
    "    # Generate log prior for metastructures \n",
    "    lpm = [(logprior(x, num_monomers)).tolist() for x in templates_flatten]\n",
    "    \n",
    "    # Generate log likelihood for observations given metastructures \n",
    "    ll = [[(loglikelihood(y, x, measurement_error, num_monomers)).tolist() for x in templates_flatten] for y in observation_flatten_list]\n",
    "    \n",
    "    N = num_observations\n",
    "    M = num_candidates\n",
    "    \n",
    "    # Load stan model \n",
    "    my_model = CmdStanModel(\n",
    "        stan_file='/mnt/home/tudomlumleart/ceph/01_ChromatinEnsembleRefinement/chromatin-ensemble-refinement/scripts/stan/20240715_WeightOptimization.stan',\n",
    "        cpp_options = {\n",
    "            \"STAN_THREADS\": True,\n",
    "        }\n",
    "        )\n",
    "    \n",
    "    n_cores = multiprocessing.cpu_count()\n",
    "    print(f\"Number of CPU cores: {n_cores}\")\n",
    "    parallel_chains = 4\n",
    "    threads_per_chain = int(n_cores / parallel_chains)\n",
    "    print(f\"Number of threads per chain: {threads_per_chain}\")\n",
    "    \n",
    "    output_dir = os.path.join(save_dir, str(dataset_idx))\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "        \n",
    "    # Write json files for reading into stan program\n",
    "    json_filename = os.path.join(output_dir, 'data.json')\n",
    "    stan_output_file = os.path.join(output_dir, 'stan_output')\n",
    "    data_dict = {\n",
    "        \"M\": M,\n",
    "        \"N\": N,\n",
    "        \"ll_map\": ll,\n",
    "        \"lpm_vec\": lpm,\n",
    "    }\n",
    "\n",
    "    json_obj = json.dumps(data_dict, indent=4)\n",
    "\n",
    "    with open(json_filename, 'w') as json_file:\n",
    "        json_file.write(json_obj)\n",
    "        json_file.close()\n",
    "        \n",
    "    # Run Stan model to perform MCMC sampling\n",
    "    data_file = json_filename\n",
    "    \n",
    "    fit = my_model.sample(\n",
    "        data=data_file,\n",
    "        chains=4,\n",
    "        sig_figs=8,\n",
    "        parallel_chains=parallel_chains,\n",
    "        threads_per_chain=threads_per_chain,\n",
    "        iter_warmup=1000,\n",
    "        iter_sampling=1000,\n",
    "        show_console=True,\n",
    "    )\n",
    "        \n",
    "    # Save Stan output, i.e., posterior samples, in CSV format, in a specified folder\n",
    "    fit.save_csvfiles(dir=stan_output_file)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33aa6e5aecfc44c283fb3200e6365d83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running MCMC for dataset 0...\n",
      "Number of CPU cores: 40\n",
      "Number of threads per chain: 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15:34:35 - cmdstanpy - INFO - CmdStan start processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "method = sample (Default)\n",
      "sample\n",
      "num_samples = 1000 (Default)\n",
      "num_warmup = 1000 (Default)\n",
      "save_warmup = false (Default)\n",
      "thin = 1 (Default)\n",
      "adapt\n",
      "engaged = true (Default)\n",
      "gamma = 0.05 (Default)\n",
      "delta = 0.8 (Default)\n",
      "kappa = 0.75 (Default)\n",
      "t0 = 10 (Default)\n",
      "init_buffer = 75 (Default)\n",
      "term_buffer = 50 (Default)\n",
      "window = 25 (Default)\n",
      "save_metric = false (Default)\n",
      "algorithm = hmc (Default)\n",
      "hmc\n",
      "engine = nuts (Default)\n",
      "nuts\n",
      "max_depth = 10 (Default)\n",
      "metric = diag_e (Default)\n",
      "metric_file =  (Default)\n",
      "stepsize = 1 (Default)\n",
      "stepsize_jitter = 0 (Default)\n",
      "num_chains = 4\n",
      "id = 1 (Default)\n",
      "data\n",
      "file = /mnt/home/tudomlumleart/ceph/01_ChromatinEnsembleRefinement/chromatin-ensemble-refinement/MCMC_results/dataset_100_10_20_10_0_10/0/data.json\n",
      "init = 2 (Default)\n",
      "random\n",
      "seed = 37808\n",
      "output\n",
      "file = /tmp/tmp6k_ia21x/20240715_WeightOptimizationyobpovlf/20240715_WeightOptimization-20240715153435.csv\n",
      "diagnostic_file =  (Default)\n",
      "refresh = 100 (Default)\n",
      "sig_figs = 8\n",
      "profile_file = profile.csv (Default)\n",
      "save_cmdstan_config = false (Default)\n",
      "num_threads = 40 (Default)\n",
      "\n",
      "\n",
      "Gradient evaluation took 0.000967 seconds\n",
      "1000 transitions using 10 leapfrog steps per transition would take 9.67 seconds.\n",
      "Adjust your expectations accordingly!\n",
      "\n",
      "\n",
      "\n",
      "Gradient evaluation took 0.001018 seconds\n",
      "1000 transitions using 10 leapfrog steps per transition would take 10.18 seconds.\n",
      "Adjust your expectations accordingly!\n",
      "\n",
      "\n",
      "\n",
      "Gradient evaluation took 0.000997 seconds\n",
      "1000 transitions using 10 leapfrog steps per transition would take 9.97 seconds.\n",
      "Adjust your expectations accordingly!\n",
      "\n",
      "\n",
      "\n",
      "Gradient evaluation took 0.000978 seconds\n",
      "1000 transitions using 10 leapfrog steps per transition would take 9.78 seconds.\n",
      "Adjust your expectations accordingly!\n",
      "\n",
      "\n",
      "Chain [4] Iteration:    1 / 2000 [  0%]  (Warmup)\n",
      "Chain [2] Iteration:    1 / 2000 [  0%]  (Warmup)\n",
      "Chain [1] Iteration:    1 / 2000 [  0%]  (Warmup)\n",
      "Chain [3] Iteration:    1 / 2000 [  0%]  (Warmup)\n",
      "Chain [1] Iteration:  100 / 2000 [  5%]  (Warmup)\n",
      "Chain [3] Iteration:  100 / 2000 [  5%]  (Warmup)\n",
      "Chain [2] Iteration:  100 / 2000 [  5%]  (Warmup)\n",
      "Chain [4] Iteration:  100 / 2000 [  5%]  (Warmup)\n",
      "Chain [3] Iteration:  200 / 2000 [ 10%]  (Warmup)\n",
      "Chain [2] Iteration:  200 / 2000 [ 10%]  (Warmup)\n",
      "Chain [1] Iteration:  200 / 2000 [ 10%]  (Warmup)\n",
      "Chain [4] Iteration:  200 / 2000 [ 10%]  (Warmup)\n",
      "Chain [3] Iteration:  300 / 2000 [ 15%]  (Warmup)\n",
      "Chain [2] Iteration:  300 / 2000 [ 15%]  (Warmup)\n",
      "Chain [1] Iteration:  300 / 2000 [ 15%]  (Warmup)\n",
      "Chain [4] Iteration:  300 / 2000 [ 15%]  (Warmup)\n",
      "Chain [3] Iteration:  400 / 2000 [ 20%]  (Warmup)\n",
      "Chain [2] Iteration:  400 / 2000 [ 20%]  (Warmup)\n",
      "Chain [1] Iteration:  400 / 2000 [ 20%]  (Warmup)\n",
      "Chain [4] Iteration:  400 / 2000 [ 20%]  (Warmup)\n",
      "Chain [3] Iteration:  500 / 2000 [ 25%]  (Warmup)\n",
      "Chain [2] Iteration:  500 / 2000 [ 25%]  (Warmup)\n",
      "Chain [1] Iteration:  500 / 2000 [ 25%]  (Warmup)\n",
      "Chain [4] Iteration:  500 / 2000 [ 25%]  (Warmup)\n",
      "Chain [3] Iteration:  600 / 2000 [ 30%]  (Warmup)\n",
      "Chain [2] Iteration:  600 / 2000 [ 30%]  (Warmup)\n",
      "Chain [1] Iteration:  600 / 2000 [ 30%]  (Warmup)\n",
      "Chain [4] Iteration:  600 / 2000 [ 30%]  (Warmup)\n",
      "Chain [3] Iteration:  700 / 2000 [ 35%]  (Warmup)\n",
      "Chain [2] Iteration:  700 / 2000 [ 35%]  (Warmup)\n",
      "Chain [1] Iteration:  700 / 2000 [ 35%]  (Warmup)\n",
      "Chain [4] Iteration:  700 / 2000 [ 35%]  (Warmup)\n",
      "Chain [3] Iteration:  800 / 2000 [ 40%]  (Warmup)\n",
      "Chain [2] Iteration:  800 / 2000 [ 40%]  (Warmup)\n",
      "Chain [1] Iteration:  800 / 2000 [ 40%]  (Warmup)\n",
      "Chain [4] Iteration:  800 / 2000 [ 40%]  (Warmup)\n",
      "Chain [3] Iteration:  900 / 2000 [ 45%]  (Warmup)\n",
      "Chain [2] Iteration:  900 / 2000 [ 45%]  (Warmup)\n",
      "Chain [1] Iteration:  900 / 2000 [ 45%]  (Warmup)\n",
      "Chain [4] Iteration:  900 / 2000 [ 45%]  (Warmup)\n",
      "Chain [3] Iteration: 1000 / 2000 [ 50%]  (Warmup)\n",
      "Chain [3] Iteration: 1001 / 2000 [ 50%]  (Sampling)\n",
      "Chain [2] Iteration: 1000 / 2000 [ 50%]  (Warmup)\n",
      "Chain [2] Iteration: 1001 / 2000 [ 50%]  (Sampling)\n",
      "Chain [1] Iteration: 1000 / 2000 [ 50%]  (Warmup)\n",
      "Chain [1] Iteration: 1001 / 2000 [ 50%]  (Sampling)\n",
      "Chain [4] Iteration: 1000 / 2000 [ 50%]  (Warmup)\n",
      "Chain [4] Iteration: 1001 / 2000 [ 50%]  (Sampling)\n",
      "Chain [3] Iteration: 1100 / 2000 [ 55%]  (Sampling)\n",
      "Chain [2] Iteration: 1100 / 2000 [ 55%]  (Sampling)\n",
      "Chain [1] Iteration: 1100 / 2000 [ 55%]  (Sampling)\n",
      "Chain [4] Iteration: 1100 / 2000 [ 55%]  (Sampling)\n",
      "Chain [3] Iteration: 1200 / 2000 [ 60%]  (Sampling)\n",
      "Chain [2] Iteration: 1200 / 2000 [ 60%]  (Sampling)\n",
      "Chain [1] Iteration: 1200 / 2000 [ 60%]  (Sampling)\n",
      "Chain [4] Iteration: 1200 / 2000 [ 60%]  (Sampling)\n",
      "Chain [3] Iteration: 1300 / 2000 [ 65%]  (Sampling)\n",
      "Chain [2] Iteration: 1300 / 2000 [ 65%]  (Sampling)\n",
      "Chain [1] Iteration: 1300 / 2000 [ 65%]  (Sampling)\n",
      "Chain [4] Iteration: 1300 / 2000 [ 65%]  (Sampling)\n",
      "Chain [3] Iteration: 1400 / 2000 [ 70%]  (Sampling)\n",
      "Chain [2] Iteration: 1400 / 2000 [ 70%]  (Sampling)\n",
      "Chain [1] Iteration: 1400 / 2000 [ 70%]  (Sampling)\n",
      "Chain [4] Iteration: 1400 / 2000 [ 70%]  (Sampling)\n",
      "Chain [3] Iteration: 1500 / 2000 [ 75%]  (Sampling)\n",
      "Chain [2] Iteration: 1500 / 2000 [ 75%]  (Sampling)\n",
      "Chain [1] Iteration: 1500 / 2000 [ 75%]  (Sampling)\n",
      "Chain [4] Iteration: 1500 / 2000 [ 75%]  (Sampling)\n",
      "Chain [3] Iteration: 1600 / 2000 [ 80%]  (Sampling)\n",
      "Chain [2] Iteration: 1600 / 2000 [ 80%]  (Sampling)\n",
      "Chain [1] Iteration: 1600 / 2000 [ 80%]  (Sampling)\n",
      "Chain [4] Iteration: 1600 / 2000 [ 80%]  (Sampling)\n",
      "Chain [2] Iteration: 1700 / 2000 [ 85%]  (Sampling)\n",
      "Chain [3] Iteration: 1700 / 2000 [ 85%]  (Sampling)\n",
      "Chain [1] Iteration: 1700 / 2000 [ 85%]  (Sampling)\n",
      "Chain [4] Iteration: 1700 / 2000 [ 85%]  (Sampling)\n",
      "Chain [2] Iteration: 1800 / 2000 [ 90%]  (Sampling)\n",
      "Chain [3] Iteration: 1800 / 2000 [ 90%]  (Sampling)\n",
      "Chain [1] Iteration: 1800 / 2000 [ 90%]  (Sampling)\n",
      "Chain [2] Iteration: 1900 / 2000 [ 95%]  (Sampling)\n",
      "Chain [4] Iteration: 1800 / 2000 [ 90%]  (Sampling)\n",
      "Chain [3] Iteration: 1900 / 2000 [ 95%]  (Sampling)\n",
      "Chain [1] Iteration: 1900 / 2000 [ 95%]  (Sampling)\n",
      "Chain [2] Iteration: 2000 / 2000 [100%]  (Sampling)\n",
      "\n",
      "Elapsed Time: 23.205 seconds (Warm-up)\n",
      "23.028 seconds (Sampling)\n",
      "46.233 seconds (Total)\n",
      "\n",
      "Chain [4] Iteration: 1900 / 2000 [ 95%]  (Sampling)\n",
      "Chain [3] Iteration: 2000 / 2000 [100%]  (Sampling)\n",
      "\n",
      "Elapsed Time: 22.449 seconds (Warm-up)\n",
      "24.212 seconds (Sampling)\n",
      "46.661 seconds (Total)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15:35:22 - cmdstanpy - INFO - CmdStan done processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chain [1] Iteration: 2000 / 2000 [100%]  (Sampling)\n",
      "\n",
      "Elapsed Time: 23.427 seconds (Warm-up)\n",
      "23.974 seconds (Sampling)\n",
      "47.401 seconds (Total)\n",
      "\n",
      "Chain [4] Iteration: 2000 / 2000 [100%]  (Sampling)\n",
      "\n",
      "Elapsed Time: 24.009 seconds (Warm-up)\n",
      "23.546 seconds (Sampling)\n",
      "47.555 seconds (Total)\n",
      "\n",
      "\n",
      "Running MCMC for dataset 1...\n",
      "Number of CPU cores: 40\n",
      "Number of threads per chain: 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15:35:31 - cmdstanpy - INFO - CmdStan start processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "method = sample (Default)\n",
      "sample\n",
      "num_samples = 1000 (Default)\n",
      "num_warmup = 1000 (Default)\n",
      "save_warmup = false (Default)\n",
      "thin = 1 (Default)\n",
      "adapt\n",
      "engaged = true (Default)\n",
      "gamma = 0.05 (Default)\n",
      "delta = 0.8 (Default)\n",
      "kappa = 0.75 (Default)\n",
      "t0 = 10 (Default)\n",
      "init_buffer = 75 (Default)\n",
      "term_buffer = 50 (Default)\n",
      "window = 25 (Default)\n",
      "save_metric = false (Default)\n",
      "algorithm = hmc (Default)\n",
      "hmc\n",
      "engine = nuts (Default)\n",
      "nuts\n",
      "max_depth = 10 (Default)\n",
      "metric = diag_e (Default)\n",
      "metric_file =  (Default)\n",
      "stepsize = 1 (Default)\n",
      "stepsize_jitter = 0 (Default)\n",
      "num_chains = 4\n",
      "id = 1 (Default)\n",
      "data\n",
      "file = /mnt/home/tudomlumleart/ceph/01_ChromatinEnsembleRefinement/chromatin-ensemble-refinement/MCMC_results/dataset_100_10_20_10_0_10/1/data.json\n",
      "init = 2 (Default)\n",
      "random\n",
      "seed = 56302\n",
      "output\n",
      "file = /tmp/tmp6k_ia21x/20240715_WeightOptimization1mmiw6vv/20240715_WeightOptimization-20240715153531.csv\n",
      "diagnostic_file =  (Default)\n",
      "refresh = 100 (Default)\n",
      "sig_figs = 8\n",
      "profile_file = profile.csv (Default)\n",
      "save_cmdstan_config = false (Default)\n",
      "num_threads = 40 (Default)\n",
      "\n",
      "\n",
      "Gradient evaluation took 0.00155 seconds\n",
      "1000 transitions using 10 leapfrog steps per transition would take 15.5 seconds.\n",
      "Adjust your expectations accordingly!\n",
      "\n",
      "\n",
      "\n",
      "Gradient evaluation took 0.00114 seconds\n",
      "1000 transitions using 10 leapfrog steps per transition would take 11.4 seconds.\n",
      "Adjust your expectations accordingly!\n",
      "\n",
      "\n",
      "\n",
      "Gradient evaluation took 0.001061 seconds\n",
      "1000 transitions using 10 leapfrog steps per transition would take 10.61 seconds.\n",
      "Adjust your expectations accordingly!\n",
      "\n",
      "\n",
      "\n",
      "Gradient evaluation took 0.001108 seconds\n",
      "1000 transitions using 10 leapfrog steps per transition would take 11.08 seconds.\n",
      "Adjust your expectations accordingly!\n",
      "\n",
      "\n",
      "Chain [2] Iteration:    1 / 2000 [  0%]  (Warmup)\n",
      "Chain [1] Iteration:    1 / 2000 [  0%]  (Warmup)\n",
      "Chain [4] Iteration:    1 / 2000 [  0%]  (Warmup)\n",
      "Chain [3] Iteration:    1 / 2000 [  0%]  (Warmup)\n",
      "Chain [2] Iteration:  100 / 2000 [  5%]  (Warmup)\n",
      "Chain [4] Iteration:  100 / 2000 [  5%]  (Warmup)\n",
      "Chain [3] Iteration:  100 / 2000 [  5%]  (Warmup)\n",
      "Chain [1] Iteration:  100 / 2000 [  5%]  (Warmup)\n",
      "Chain [1] Iteration:  200 / 2000 [ 10%]  (Warmup)\n",
      "Chain [4] Iteration:  200 / 2000 [ 10%]  (Warmup)\n",
      "Chain [3] Iteration:  200 / 2000 [ 10%]  (Warmup)\n",
      "Chain [2] Iteration:  200 / 2000 [ 10%]  (Warmup)\n",
      "Chain [3] Iteration:  300 / 2000 [ 15%]  (Warmup)\n",
      "Chain [4] Iteration:  300 / 2000 [ 15%]  (Warmup)\n",
      "Chain [2] Iteration:  300 / 2000 [ 15%]  (Warmup)\n",
      "Chain [1] Iteration:  300 / 2000 [ 15%]  (Warmup)\n",
      "Chain [3] Iteration:  400 / 2000 [ 20%]  (Warmup)\n",
      "Chain [2] Iteration:  400 / 2000 [ 20%]  (Warmup)\n",
      "Chain [4] Iteration:  400 / 2000 [ 20%]  (Warmup)\n",
      "Chain [1] Iteration:  400 / 2000 [ 20%]  (Warmup)\n",
      "Chain [3] Iteration:  500 / 2000 [ 25%]  (Warmup)\n",
      "Chain [4] Iteration:  500 / 2000 [ 25%]  (Warmup)\n",
      "Chain [1] Iteration:  500 / 2000 [ 25%]  (Warmup)\n",
      "Chain [2] Iteration:  500 / 2000 [ 25%]  (Warmup)\n",
      "Chain [3] Iteration:  600 / 2000 [ 30%]  (Warmup)\n",
      "Chain [4] Iteration:  600 / 2000 [ 30%]  (Warmup)\n",
      "Chain [1] Iteration:  600 / 2000 [ 30%]  (Warmup)\n",
      "Chain [2] Iteration:  600 / 2000 [ 30%]  (Warmup)\n",
      "Chain [3] Iteration:  700 / 2000 [ 35%]  (Warmup)\n",
      "Chain [4] Iteration:  700 / 2000 [ 35%]  (Warmup)\n",
      "Chain [1] Iteration:  700 / 2000 [ 35%]  (Warmup)\n",
      "Chain [2] Iteration:  700 / 2000 [ 35%]  (Warmup)\n",
      "Chain [3] Iteration:  800 / 2000 [ 40%]  (Warmup)\n",
      "Chain [4] Iteration:  800 / 2000 [ 40%]  (Warmup)\n",
      "Chain [2] Iteration:  800 / 2000 [ 40%]  (Warmup)\n",
      "Chain [1] Iteration:  800 / 2000 [ 40%]  (Warmup)\n",
      "Chain [3] Iteration:  900 / 2000 [ 45%]  (Warmup)\n",
      "Chain [4] Iteration:  900 / 2000 [ 45%]  (Warmup)\n",
      "Chain [2] Iteration:  900 / 2000 [ 45%]  (Warmup)\n",
      "Chain [1] Iteration:  900 / 2000 [ 45%]  (Warmup)\n",
      "Chain [3] Iteration: 1000 / 2000 [ 50%]  (Warmup)\n",
      "Chain [3] Iteration: 1001 / 2000 [ 50%]  (Sampling)\n",
      "Chain [4] Iteration: 1000 / 2000 [ 50%]  (Warmup)\n",
      "Chain [4] Iteration: 1001 / 2000 [ 50%]  (Sampling)\n",
      "Chain [2] Iteration: 1000 / 2000 [ 50%]  (Warmup)\n",
      "Chain [2] Iteration: 1001 / 2000 [ 50%]  (Sampling)\n",
      "Chain [1] Iteration: 1000 / 2000 [ 50%]  (Warmup)\n",
      "Chain [1] Iteration: 1001 / 2000 [ 50%]  (Sampling)\n",
      "Chain [4] Iteration: 1100 / 2000 [ 55%]  (Sampling)\n",
      "Chain [3] Iteration: 1100 / 2000 [ 55%]  (Sampling)\n",
      "Chain [2] Iteration: 1100 / 2000 [ 55%]  (Sampling)\n",
      "Chain [1] Iteration: 1100 / 2000 [ 55%]  (Sampling)\n",
      "Chain [4] Iteration: 1200 / 2000 [ 60%]  (Sampling)\n",
      "Chain [2] Iteration: 1200 / 2000 [ 60%]  (Sampling)\n",
      "Chain [3] Iteration: 1200 / 2000 [ 60%]  (Sampling)\n",
      "Chain [1] Iteration: 1200 / 2000 [ 60%]  (Sampling)\n",
      "Chain [4] Iteration: 1300 / 2000 [ 65%]  (Sampling)\n",
      "Chain [2] Iteration: 1300 / 2000 [ 65%]  (Sampling)\n",
      "Chain [3] Iteration: 1300 / 2000 [ 65%]  (Sampling)\n",
      "Chain [1] Iteration: 1300 / 2000 [ 65%]  (Sampling)\n",
      "Chain [4] Iteration: 1400 / 2000 [ 70%]  (Sampling)\n",
      "Chain [2] Iteration: 1400 / 2000 [ 70%]  (Sampling)\n",
      "Chain [3] Iteration: 1400 / 2000 [ 70%]  (Sampling)\n",
      "Chain [1] Iteration: 1400 / 2000 [ 70%]  (Sampling)\n",
      "Chain [2] Iteration: 1500 / 2000 [ 75%]  (Sampling)\n",
      "Chain [4] Iteration: 1500 / 2000 [ 75%]  (Sampling)\n",
      "Chain [3] Iteration: 1500 / 2000 [ 75%]  (Sampling)\n",
      "Chain [1] Iteration: 1500 / 2000 [ 75%]  (Sampling)\n",
      "Chain [2] Iteration: 1600 / 2000 [ 80%]  (Sampling)\n",
      "Chain [4] Iteration: 1600 / 2000 [ 80%]  (Sampling)\n",
      "Chain [3] Iteration: 1600 / 2000 [ 80%]  (Sampling)\n",
      "Chain [1] Iteration: 1600 / 2000 [ 80%]  (Sampling)\n",
      "Chain [2] Iteration: 1700 / 2000 [ 85%]  (Sampling)\n",
      "Chain [4] Iteration: 1700 / 2000 [ 85%]  (Sampling)\n",
      "Chain [3] Iteration: 1700 / 2000 [ 85%]  (Sampling)\n",
      "Chain [2] Iteration: 1800 / 2000 [ 90%]  (Sampling)\n",
      "Chain [1] Iteration: 1700 / 2000 [ 85%]  (Sampling)\n",
      "Chain [4] Iteration: 1800 / 2000 [ 90%]  (Sampling)\n",
      "Chain [3] Iteration: 1800 / 2000 [ 90%]  (Sampling)\n",
      "Chain [2] Iteration: 1900 / 2000 [ 95%]  (Sampling)\n",
      "Chain [4] Iteration: 1900 / 2000 [ 95%]  (Sampling)\n",
      "Chain [1] Iteration: 1800 / 2000 [ 90%]  (Sampling)\n",
      "Chain [3] Iteration: 1900 / 2000 [ 95%]  (Sampling)\n",
      "Chain [2] Iteration: 2000 / 2000 [100%]  (Sampling)\n",
      "\n",
      "Elapsed Time: 25.15 seconds (Warm-up)\n",
      "22.133 seconds (Sampling)\n",
      "47.283 seconds (Total)\n",
      "\n",
      "Chain [4] Iteration: 2000 / 2000 [100%]  (Sampling)\n",
      "\n",
      "Elapsed Time: 24.718 seconds (Warm-up)\n",
      "23.157 seconds (Sampling)\n",
      "47.875 seconds (Total)\n",
      "\n",
      "Chain [1] Iteration: 1900 / 2000 [ 95%]  (Sampling)\n",
      "Chain [3] Iteration: 2000 / 2000 [100%]  (Sampling)\n",
      "\n",
      "Elapsed Time: 24.606 seconds (Warm-up)\n",
      "24.031 seconds (Sampling)\n",
      "48.637 seconds (Total)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15:36:20 - cmdstanpy - INFO - CmdStan done processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chain [1] Iteration: 2000 / 2000 [100%]  (Sampling)\n",
      "\n",
      "Elapsed Time: 25.416 seconds (Warm-up)\n",
      "23.573 seconds (Sampling)\n",
      "48.989 seconds (Total)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "p = '/mnt/home/tudomlumleart/ceph/03_GaussianChainSimulation/20240627/dataset_100_10_20_10_0_10.0_10000.pkl'\n",
    "# Load the dataset from that pickle file \n",
    "print(\"Loading dataset...\")\n",
    "dataset_list, param_dict = load_dataset(p)\n",
    "\n",
    "# Generate save directory for the result \n",
    "# Extracting the core name of the pickle file\n",
    "core_name = p.split(\"/\")[-1].split(\".\")[0]\n",
    "mcmc_result_dir = '/mnt/home/tudomlumleart/ceph/01_ChromatinEnsembleRefinement/chromatin-ensemble-refinement/MCMC_results'\n",
    "save_dir = os.path.join(mcmc_result_dir, core_name)\n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "    \n",
    "# Run the MCMC for 100 datasets in the list\\\n",
    "for i in tqdm(range(2)):\n",
    "    print(f\"Running MCMC for dataset {i}...\")\n",
    "    run_mcmc(dataset_list, param_dict, save_dir, i)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyter-gpu-openmm",
   "language": "python",
   "name": "jupyter-gpu-openmm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
