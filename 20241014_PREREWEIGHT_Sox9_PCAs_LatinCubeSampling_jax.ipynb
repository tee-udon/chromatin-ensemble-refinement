{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from _utils import *\n",
    "from scipy.stats import qmc\n",
    "\n",
    "def calculate_conformational_variance_jax(dmap_list, dmap_ref):\n",
    "    \"\"\"\n",
    "    Calculate the conformational variation of a set of distance maps relative to a reference map.\n",
    "\n",
    "    Parameters:\n",
    "    dmap_list (list): A list of 2D numpy arrays representing the distance maps.\n",
    "    dmap_ref (np.ndarray): A 2D numpy array representing the reference distance map.\n",
    "    num_probes (int): The number of probes in the distance maps.\n",
    "\n",
    "    Returns:\n",
    "    np.ndarray: A 2D numpy array containing the variance of the squared Euclidean distances \n",
    "               between each distance map and the reference map.\n",
    "    \"\"\"\n",
    "    # Convert dmap_list to a NumPy array\n",
    "    dmap_list = jnp.array(dmap_list)\n",
    "    \n",
    "    # Calculate the squared Euclidean distance between each distance map and the reference map\n",
    "    diff_list = jnp.sqrt((dmap_list - dmap_ref) ** 2) \n",
    "    \n",
    "    # Calculate the variance along the number of observation/cell dimension\n",
    "    var = jnp.var(diff_list, axis=0)\n",
    "    \n",
    "    return var\n",
    "\n",
    "\n",
    "# Rewrite this in a jax-compatible fashion\n",
    "from functools import partial\n",
    "@partial(jax.jit, static_argnums=(2,)) \n",
    "def batch_calculate_variances(dmap_list, dmap_ref, num_probes):\n",
    "    \"\"\"\n",
    "    Vectorized version that applies calculate_conformational_variance_jax across a batch of distance maps.\n",
    "    \"\"\"\n",
    "    return jax.vmap(lambda dmap: calculate_conformational_variance_jax(dmap_list, jnp.reshape(dmap, [num_probes, num_probes])))(dmap_ref)\n",
    "\n",
    "\n",
    "# Define the main loglikelihood function using JAX\n",
    "def loglikelihood_jax(dmap_flat, ref_dmap_flat, measurement_error, num_probes):\n",
    "    return jnp.sum(_loglikelihood_jax(dmap_flat, ref_dmap_flat, measurement_error, num_probes))\n",
    "\n",
    "\n",
    "# Define the helper function, with JAX-compatible logic\n",
    "def _loglikelihood_jax(dmap_flat, ref_dmap_flat, measurement_error, num_probes):\n",
    "    # Use lax.cond for control flow based on the condition\n",
    "    min_value = jnp.iinfo(jnp.int32).min\n",
    "    \n",
    "    def handle_invalid_reference(ref_dmap_flat):\n",
    "        # Return extremely low probability when ref_dmap_flat contains invalid values\n",
    "        return jnp.array([jnp.float32(min_value), jnp.float32(min_value)])\n",
    "    \n",
    "    def handle_valid_reference(ref_dmap_flat):\n",
    "        # Calculate the difference between distance map and reference \n",
    "        subtraction_map_sq = jnp.square(dmap_flat - ref_dmap_flat).reshape(num_probes, num_probes)\n",
    "\n",
    "        # Only consider the upper triangular part of the distance map\n",
    "        # because the diagonal values do not have variance\n",
    "        triu_indices = jnp.triu_indices(num_probes, k=1)\n",
    "        measurement_error_scaled = 2 * measurement_error[triu_indices]  # both triangles \n",
    "        subtraction_map_sq_scaled = 2 * subtraction_map_sq[triu_indices]  # both triangles\n",
    "        \n",
    "        # Calculate the normalization factor\n",
    "        normalization_factor = -jnp.sum(jnp.log(jnp.sqrt(2 * jnp.pi * measurement_error_scaled**2)))\n",
    "        \n",
    "        # Calculate the Gaussian term \n",
    "        gaussian_term = -jnp.sum(subtraction_map_sq_scaled / (2 * jnp.square(measurement_error_scaled)))\n",
    "        \n",
    "        return jnp.array([normalization_factor, gaussian_term])\n",
    "\n",
    "    # Apply the appropriate logic depending on whether ref_dmap_flat contains negative values\n",
    "    return lax.cond(\n",
    "        jnp.any(ref_dmap_flat <= -1),\n",
    "        handle_invalid_reference,\n",
    "        handle_valid_reference,\n",
    "        ref_dmap_flat\n",
    "    )\n",
    "    \n",
    "def compute_loglikelihood_for_y(y, templates_flatten, measurement_error_esc, num_probes):\n",
    "    return jax.vmap(lambda x, z: loglikelihood_jax(y, x, z, num_probes))(templates_flatten, measurement_error_esc)\n",
    "\n",
    "\n",
    "folder_path = '/mnt/home/tudomlumleart/ceph/05_Sox9Dataset/'\n",
    "    \n",
    "num_monomers = 80\n",
    "\n",
    "# Load polys data and then perform linear interpolation\n",
    "# List all .mat files in the folder and load them\n",
    "cnc_polys = scipy.io.loadmat(folder_path + 'cncPols.mat')['cncPols'][:num_monomers, :, :]\n",
    "esc_polys = scipy.io.loadmat(folder_path + 'escPols.mat')['escPols'][:num_monomers, :, :]\n",
    "\n",
    "esc_polys_interp = interpolate_polymers(esc_polys)\n",
    "cnc_polys_interp = interpolate_polymers(cnc_polys)\n",
    "\n",
    "def calculate_distance_map(polys):\n",
    "    # Extract the dimensions of the input array\n",
    "    num_probes, num_coords, num_cells = polys.shape\n",
    "    \n",
    "    # Initialize an array of the same shape to hold the interpolated values\n",
    "    new_maps = np.zeros((num_cells, num_probes, num_probes))\n",
    "    \n",
    "    # Iterate over each cell\n",
    "    for c in range(num_cells):\n",
    "        # Extract the data for the current cell\n",
    "        curr_cells = polys[:, :, c]\n",
    "        \n",
    "        # Skip cells with all missing values\n",
    "        if np.all(np.isnan(curr_cells)):\n",
    "            continue  # This leaves a matrix of zeros in the output array\n",
    "        \n",
    "        # Calculate the pairwise Euclidean distance between each pair of probes\n",
    "        dmap = squareform(pdist(curr_cells))\n",
    "        \n",
    "        # Assign the distance map to the corresponding position in the output array\n",
    "        new_maps[c, :, :] = dmap\n",
    "    \n",
    "    # Return the array with interpolated values\n",
    "    return new_maps\n",
    "\n",
    "esc_maps_interp = calculate_distance_map(esc_polys_interp)\n",
    "cnc_maps_interp = calculate_distance_map(cnc_polys_interp)\n",
    "esc_maps_interp_flat = np.array([x.flatten() for x in esc_maps_interp])\n",
    "cnc_maps_interp_flat = np.array([x.flatten() for x in cnc_maps_interp])\n",
    "all_maps_interp = np.concatenate((esc_maps_interp, cnc_maps_interp), axis=0)\n",
    "all_maps_interp_flat = np.concatenate((esc_maps_interp_flat, cnc_maps_interp_flat), axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pca = PCA(n_components=2)  # Change here for the number of dimensions\n",
    "pca.fit(all_maps_interp_flat)\n",
    "esc_maps_pca = pca.transform(esc_maps_interp_flat)\n",
    "cnc_maps_pca = pca.transform(cnc_maps_interp_flat)\n",
    "\n",
    "all_maps_pca = np.concatenate((esc_maps_pca, cnc_maps_pca), axis=0)\n",
    "\n",
    "# Use Latin Hypercube sampling to generate 1000 samples\n",
    "# This technique tries to sample data points in a way that they are evenly distributed\n",
    "\n",
    "num_microstates = 1000\n",
    "num_dimension = pca.n_components # pca.n_components_\n",
    "\n",
    "sampler = qmc.LatinHypercube(d=num_dimension)\n",
    "sample = sampler.random(n=num_microstates)\n",
    "\n",
    "# We can use quantiles to get physical bounds\n",
    "l_quantile = 0.01\n",
    "u_quantile = 1 - l_quantile\n",
    "l_bounds = np.quantile(all_maps_pca, l_quantile, axis=0)\n",
    "u_bounds = np.quantile(all_maps_pca, u_quantile, axis=0)\n",
    "sample_scaled = qmc.scale(sample, l_bounds, u_bounds)\n",
    "\n",
    "# Add these sample_scaled coordinates to a dataframe\n",
    "sample_df = pd.DataFrame(sample_scaled, columns=[f'PC{i+1}' for i in range(pca.n_components_)])\n",
    "\n",
    "# Inverse transform the samples\n",
    "microstates_maps = pca.inverse_transform(sample_scaled)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf; print(tf.config.list_physical_devices('GPU'))  # This prevents crash on GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "microstates_maps_jax = jnp.array(microstates_maps)\n",
    "esc_std = batch_calculate_variances(jnp.array(esc_maps_interp), microstates_maps_jax, num_monomers)**0.5\n",
    "cnc_std = batch_calculate_variances(jnp.array(cnc_maps_interp), microstates_maps_jax, num_monomers)**0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "lpm = [(logprior(x, num_monomers)).tolist() for x in microstates_maps]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lpm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b211856693e4fd4b92fa12275247c24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1225 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18a0867be36a41cda59d96cea6c1649a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1757 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "esc_ll = [compute_loglikelihood_for_y(y, microstates_maps_jax, esc_std, num_monomers) for y in tqdm(esc_maps_interp_flat)]\n",
    "cnc_ll = [compute_loglikelihood_for_y(y, microstates_maps_jax, cnc_std, num_monomers) for y in tqdm(cnc_maps_interp_flat)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1225, 1000)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(esc_ll).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of CPU cores: 64\n",
      "Number of threads per chain: 16\n"
     ]
    }
   ],
   "source": [
    "esc_ll = [x.tolist() for x in esc_ll]\n",
    "cnc_ll = [x.tolist() for x in cnc_ll]\n",
    "\n",
    "N_esc = esc_maps_interp_flat.shape[0]\n",
    "N_cnc = cnc_maps_interp_flat.shape[0]\n",
    "\n",
    "M = num_microstates\n",
    "\n",
    "my_model = CmdStanModel(\n",
    "    stan_file='/mnt/home/tudomlumleart/ceph/01_ChromatinEnsembleRefinement/chromatin-ensemble-refinement/scripts/stan/20240715_WeightOptimization.stan',\n",
    "    cpp_options = {\n",
    "        \"STAN_THREADS\": True,\n",
    "    }\n",
    "    )\n",
    "\n",
    "n_cores = multiprocessing.cpu_count()\n",
    "print(f\"Number of CPU cores: {n_cores}\")\n",
    "parallel_chains = 4\n",
    "threads_per_chain = int(n_cores / parallel_chains)\n",
    "print(f\"Number of threads per chain: {threads_per_chain}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "138bff6f37d147b788ff118392093c27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "save_dir = f'/mnt/home/tudomlumleart/ceph/01_ChromatinEnsembleRefinement/chromatin-ensemble-refinement/MCMC_results/20241014_RunWeightMCMC_Sox9_PCA_LatinCubeSampling_10000_Rank{num_dimension}'\n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "    \n",
    "# save sample df to save_dir\n",
    "sample_df.to_csv(os.path.join(save_dir, 'sampled_microstates.csv'), index=False)\n",
    "\n",
    "conditions = [\n",
    "    'ESC', 'CNC'\n",
    "]\n",
    "\n",
    "for condition in tqdm(conditions):\n",
    "    output_dir = os.path.join(save_dir, condition)\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "        \n",
    "    json_filename = os.path.join(output_dir, 'data.json')\n",
    "    stan_putput_file = os.path.join(output_dir, 'stan_output')\n",
    "    \n",
    "    if condition == 'ESC':\n",
    "        data_dict = {\n",
    "            'N': N_esc,\n",
    "            'M': M,\n",
    "            'll_map': esc_ll,\n",
    "            'lpm_vec': lpm,\n",
    "        }\n",
    "    \n",
    "    elif condition == 'CNC':\n",
    "        data_dict = {\n",
    "            'N': N_cnc,\n",
    "            'M': M,\n",
    "            'll_map': cnc_ll,\n",
    "            'lpm_vec': lpm,\n",
    "        }\n",
    "        \n",
    "    json_obj = json.dumps(data_dict, indent=4)\n",
    "    \n",
    "    with open(json_filename, 'w') as json_file:\n",
    "        json_file.write(json_obj)\n",
    "        json_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def submit_mcmc_slurm(mcmc_common_dir, slurm_file):\n",
    "    # List all the directories in the MCMC common directory\n",
    "    # Only return directories that contain data.json\n",
    "    dirs = []\n",
    "\n",
    "    # List all items in the current directory\n",
    "    for item in os.listdir(mcmc_common_dir):\n",
    "        # Get the full path of the item\n",
    "        full_path = os.path.join(mcmc_common_dir, item)\n",
    "        \n",
    "        # Check if the item is a directory\n",
    "        # And if the directory contains data.json\n",
    "        # Check if 'data.json' exists in the directory\n",
    "        file_path = os.path.join(full_path, 'data.json')\n",
    "        if os.path.isfile(file_path):\n",
    "            dirs.append(full_path)\n",
    "            \n",
    "    # Submit a slurm job for each directory\n",
    "    for dir in dirs:\n",
    "        # Get the name of the directory\n",
    "        \n",
    "        dir_name = os.path.basename(dir)\n",
    "        print(f\"Submitting slurm job for {dir_name}\")\n",
    "        \n",
    "        run_mcmc_py = '/mnt/home/tudomlumleart/ceph/01_ChromatinEnsembleRefinement/chromatin-ensemble-refinement/scripts/_run_mcmc.py'\n",
    "        \n",
    "        # Submit a slurm job\n",
    "        cmd = f'sbatch {slurm_file} {run_mcmc_py} {dir}'\n",
    "        \n",
    "        # Run the command\n",
    "        os.system(cmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "slurm_file = '/mnt/home/tudomlumleart/ceph/01_ChromatinEnsembleRefinement/chromatin-ensemble-refinement/scripts/slurm/2024_RunPythonScript.sh'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitting slurm job for ESC\n",
      "Submitted batch job 4063387\n",
      "Submitting slurm job for CNC\n",
      "Submitted batch job 4063388\n"
     ]
    }
   ],
   "source": [
    "submit_mcmc_slurm(save_dir, slurm_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyter-gpu-openmm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
