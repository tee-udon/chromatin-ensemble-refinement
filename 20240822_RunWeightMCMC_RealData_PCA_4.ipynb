{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-22 15:39:33.262836: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-08-22 15:39:33.765958: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX512F AVX512_VNNI, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(r\"/mnt/ceph/users/tudomlumleart/00_VirtualEnvironments/teeu/lib/python3.10/site-packages\")\n",
    "sys.path.append(r\"/mnt/home/tudomlumleart/.local/lib/python3.10/site-packages/\")\n",
    "sys.path.append(r\"/mnt/home/tudomlumleart/ceph/00_VirtualEnvironments/jupyter-gpu/lib/python3.10/site-packages\")\n",
    "from utils import *\n",
    "from functions import *\n",
    "from sklearn.mixture import BayesianGaussianMixture, GaussianMixture \n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.model_selection import cross_val_score, cross_validate\n",
    "from sklearn.model_selection import validation_curve\n",
    "\n",
    "os.environ['TF_ENABLE_ONEDNN_OPTS']='0'\n",
    "\n",
    "from sklearn.metrics import adjusted_mutual_info_score\n",
    "\n",
    "import os\n",
    "import scipy.io\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Load dataset from file\n",
    "folder_path = '/mnt/home/tudomlumleart/ceph/05_Sox9Dataset/'\n",
    "\n",
    "# List all .mat files in the folder and load them\n",
    "cnc_maps = scipy.io.loadmat(folder_path + 'cncMaps.mat')['cncMaps'][:80, :80, :]\n",
    "esc_maps = scipy.io.loadmat(folder_path + 'escMaps.mat')['escMaps'][:80, :80, :]\n",
    "\n",
    "# Load polys data and then perform linear interpolation\n",
    "# List all .mat files in the folder and load them\n",
    "cnc_polys = scipy.io.loadmat(folder_path + 'cncPols.mat')['cncPols'][:80, :, :]\n",
    "esc_polys = scipy.io.loadmat(folder_path + 'escPols.mat')['escPols'][:80, :, :]\n",
    "\n",
    "def interpolate_polymers(polys):\n",
    "    num_probes, num_coords, num_cells = polys.shape\n",
    "    new_polys = np.zeros((num_probes, num_coords, num_cells))\n",
    "    for c in range(num_cells):\n",
    "        curr_cells = polys[:, :, c]\n",
    "        for x in range(num_coords):\n",
    "            curr_coords = curr_cells[:, x]\n",
    "            missing_indices = np.isnan(curr_coords)\n",
    "            valid_indices = ~missing_indices\n",
    "            interp_coords = np.interp(np.flatnonzero(missing_indices), np.flatnonzero(valid_indices), curr_coords[valid_indices])\n",
    "            new_polys[missing_indices, x, c] = interp_coords\n",
    "            new_polys[valid_indices, x, c] = curr_coords[valid_indices]\n",
    "    return new_polys\n",
    "\n",
    "esc_polys_interp = interpolate_polymers(esc_polys)\n",
    "cnc_polys_interp = interpolate_polymers(cnc_polys)\n",
    "\n",
    "esc_maps_interp = np.array([squareform(pdist(esc_polys_interp[:, :, i])) for i in range(esc_polys_interp.shape[2])])\n",
    "cnc_maps_interp = np.array([squareform(pdist(cnc_polys_interp[:, :, i])) for i in range(cnc_polys_interp.shape[2])])\n",
    "esc_maps_interp_flat = np.array([x.flatten() for x in esc_maps_interp])\n",
    "cnc_maps_interp_flat = np.array([x.flatten() for x in cnc_maps_interp])\n",
    "all_maps_interp_flat = np.concatenate((esc_maps_interp_flat, cnc_maps_interp_flat), axis=0)\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "pca.fit(all_maps_interp_flat)\n",
    "esc_maps_pca = pca.transform(esc_maps_interp_flat)\n",
    "cnc_maps_pca = pca.transform(cnc_maps_interp_flat)\n",
    "\n",
    "# Convert the principal components into a DataFrame\n",
    "# add a column for the label\n",
    "esc_df = pd.DataFrame(esc_maps_pca, columns=['PC1', 'PC2'])\n",
    "esc_df['label'] = 'ESC'\n",
    "cnc_df = pd.DataFrame(cnc_maps_pca, columns=['PC1', 'PC2'])\n",
    "cnc_df['label'] = 'CNC'\n",
    "all_df = pd.concat([esc_df, cnc_df], axis=0)\n",
    "\n",
    "# Find 0.05 and 0.95 quantiles of PC1 and PC2 data\n",
    "l = 0.01\n",
    "u = 1-0.01\n",
    "\n",
    "pc1_l = all_df['PC1'].quantile(l)\n",
    "pc1_u = all_df['PC1'].quantile(u)\n",
    "pc2_l = all_df['PC2'].quantile(l)\n",
    "pc2_u = all_df['PC2'].quantile(u)\n",
    "\n",
    "save_dir = '/mnt/home/tudomlumleart/ceph/01_ChromatinEnsembleRefinement/chromatin-ensemble-refinement/figures/20240812/'\n",
    "\n",
    "pc1_grid = np.linspace(pc1_l, pc1_u, 50)\n",
    "pc2_grid = np.linspace(pc2_l, pc2_u, 50)\n",
    "\n",
    "# Generate combination of pc1 and pc2 values\n",
    "pc1_grid, pc2_grid = np.meshgrid(pc1_grid, pc2_grid)\n",
    "\n",
    "# put this into a dataframe\n",
    "pc1_grid_flat = pc1_grid.flatten()\n",
    "pc2_grid_flat = pc2_grid.flatten()\n",
    "pc1_pc2_df = pd.DataFrame({'PC1': pc1_grid_flat, 'PC2': pc2_grid_flat})\n",
    "pc1_pc2_df['label'] = 'metastructures'\n",
    "\n",
    "# Sort PC2 in descending order while keeping PC1 in ascending order\n",
    "pc1_pc2_df = pc1_pc2_df.sort_values(by=['PC1', 'PC2'], ascending=[True, False], ignore_index=True)\n",
    "metastr_from_pca = pca.inverse_transform(pc1_pc2_df[['PC1', 'PC2']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_weights(directory, num_metastructures):\n",
    "    log_weights = []\n",
    "    lp = []\n",
    "    files = sorted(os.listdir(directory))[-4:]\n",
    "    print(files)\n",
    "    \n",
    "    log_weights_d = []\n",
    "    for file in files:\n",
    "        log_weights_chain = []\n",
    "        lp_chain = []\n",
    "        with open('%s/%s'%(directory, file), newline='') as csvfile:\n",
    "            reader = csv.DictReader(filter(lambda row: row[0]!='#', csvfile), )\n",
    "            for row in reader:\n",
    "                log_weights_row = [float(row[\"log_weights.%d\"%i]) for i in range(1,num_metastructures+1)]\n",
    "                lp_chain.append(float(row[\"lp__\"]))\n",
    "                log_weights_chain.append(log_weights_row)\n",
    "        log_weights = np.array(log_weights_chain)\n",
    "        lp_chain = np.array(lp_chain)\n",
    "        log_weights_d.append(log_weights)\n",
    "        lp.append(lp_chain)\n",
    "    log_weights_d = np.array(log_weights_d)\n",
    "    return log_weights_d "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['20240715_WeightOptimization-20240812175231_1.csv', '20240715_WeightOptimization-20240812175231_2.csv', '20240715_WeightOptimization-20240812175231_3.csv', '20240715_WeightOptimization-20240812175231_4.csv']\n",
      "['20240715_WeightOptimization-20240812180729_1.csv', '20240715_WeightOptimization-20240812180729_2.csv', '20240715_WeightOptimization-20240812180729_3.csv', '20240715_WeightOptimization-20240812180729_4.csv']\n"
     ]
    }
   ],
   "source": [
    "common_dir = '/mnt/home/tudomlumleart/ceph/01_ChromatinEnsembleRefinement/chromatin-ensemble-refinement/MCMC_results/20240812_WeightMCMC_PCA_metastructures_3'\n",
    "\n",
    "stan_directory_esc = os.path.join(common_dir, 'ESC/stan_output')\n",
    "stan_directory_cnc = os.path.join(common_dir, 'CNC/stan_output')\n",
    "stan_directory_all = os.path.join(common_dir, 'all/stan_output')\n",
    "\n",
    "esc_log_weights = load_weights(stan_directory_esc, 2500)\n",
    "cnc_log_weights = load_weights(stan_directory_cnc, 2500)\n",
    "\n",
    "esc_weights = np.mean(np.exp(np.array(esc_log_weights)), axis=(0, 1))\n",
    "cnc_weights = np.mean(np.exp(np.array(cnc_log_weights)), axis=(0, 1))\n",
    "\n",
    "# Put weights in a DataFrame\n",
    "esc_weights_df = pd.DataFrame({'PC1': pc1_pc2_df['PC1'].values,\n",
    "                               'PC2': pc1_pc2_df['PC1'].values,  \n",
    "                               'weight': esc_weights})\n",
    "esc_weights_df['label'] = 'ESC'\n",
    "cnc_weights_df = pd.DataFrame({'PC1': pc1_pc2_df['PC1'].values,\n",
    "                               'PC2': pc1_pc2_df['PC1'].values, \n",
    "                               'weight': cnc_weights})\n",
    "cnc_weights_df['label'] = 'CNC'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "esc_weight_table = esc_weights_df['weight'].values.reshape(50, 50).T\n",
    "cnc_weight_table = cnc_weights_df['weight'].values.reshape(50, 50).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.67"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.round(scipy.stats.entropy(esc_weights_df['weight'].values), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.54"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.round(scipy.stats.entropy(cnc_weights_df['weight'].values), 2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyter-gpu-openmm",
   "language": "python",
   "name": "jupyter-gpu-openmm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
